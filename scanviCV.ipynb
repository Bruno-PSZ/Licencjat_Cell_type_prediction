{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scvi\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 77146 × 20692\n",
       "    obs: 'barcode_name', 'Sample', 'Cluster'\n",
       "    var: 'gene_name'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.set_device(1)\n",
    "print(\"Device\", device)\n",
    "adata = ad.read_h5ad('/home/brunopsz/Data/GSE155249_COUNTS_NOT_NORMALIZED.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "sc.pp.highly_variable_genes(\n",
    "    adata,\n",
    "    flavor=\"seurat_v3\",\n",
    "    n_top_genes=5000, #spróbować na więcej może 5000\n",
    "    layer=\"counts\",\n",
    "    batch_key=\"Sample\",\n",
    "    subset=True,\n",
    ")\n",
    "\n",
    "scvi.model.SCANVI.setup_anndata(\n",
    "    adata,\n",
    "    batch_key = 'Sample',\n",
    "    layer='counts',\n",
    "    labels_key='Cluster',\n",
    "    unlabeled_category='None'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Training for \u001b[1;36m104\u001b[0m epochs.                                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/104: 100%|██████████| 104/104 [22:18<00:00, 13.46s/it, loss=1.51e+03, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=104` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/104: 100%|██████████| 104/104 [22:18<00:00, 12.87s/it, loss=1.51e+03, v_num=1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ScanVI Model with the following params: \n",
       "unlabeled_category: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, n_hidden: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, n_latent: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, n_layers: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, dropout_rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, dispersion: gene, \n",
       "gene_likelihood: zinb\n",
       "Training status: Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ScanVI Model with the following params: \n",
       "unlabeled_category: \u001b[3;35mNone\u001b[0m, n_hidden: \u001b[1;36m128\u001b[0m, n_latent: \u001b[1;36m10\u001b[0m, n_layers: \u001b[1;36m1\u001b[0m, dropout_rate: \u001b[1;36m0.1\u001b[0m, dispersion: gene, \n",
       "gene_likelihood: zinb\n",
       "Training status: Trained\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanvi = scvi.model.SCANVI(adata, n_latent = 10)\n",
    "scanvi.train()\n",
    "scanvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm[\"X_scANVI\"] = scanvi.get_latent_representation(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad('/home/brunopsz/Data/scanvi_trained_adata_on_5000_genes_latent_10.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['AT2, AT1 cells', 'B cells', 'CD4 CM T cells',\n",
      "       'CD4 cytotoxic T cells', 'CD4 prolif. T cells',\n",
      "       'CD8 cytotoxic T cells', 'CD8 cytotoxic TRM T cells',\n",
      "       'CD8 prolif. T cells', 'Ciliated cells', 'Club, Basal cells',\n",
      "       'DC1', 'DC2', 'Infected AT2, AT1 cells', 'Ionocytes', 'Mast cells',\n",
      "       'Migratory DC', 'Mixed myeloid', 'MoAM1', 'MoAM2', 'MoAM3',\n",
      "       'MoAM4', 'Plasma cells', 'Prolif. AM', 'TRAM1', 'TRAM2', 'Treg',\n",
      "       'iNKT cells', 'pDC'], dtype=object)]\n",
      "\n",
      "\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False).fit(adata.obs['Cluster'].to_numpy().reshape(-1,1))\n",
    "print(ohe.categories_)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "labels_one_hot = ohe.transform(adata.obs['Cluster'].to_numpy().reshape(-1,1))\n",
    "print(labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNAseqClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        # Initialize the modules we need to build the network\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class scRNAseqDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.label[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cell types = 28\n"
     ]
    }
   ],
   "source": [
    "cell_types = adata.obs['Cluster'].unique()\n",
    "number_of_cell_types = len(cell_types)\n",
    "print(\"Number of cell types = \" + str(number_of_cell_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9966, 0.9935, 0.9281, 0.9758, 0.9870, 0.9414, 0.9576, 0.9848, 0.9810,\n",
       "        0.9949, 0.9895, 0.9939, 0.9990, 0.9993, 0.9996, 0.9892, 0.9825, 0.9445,\n",
       "        0.8438, 0.8391, 0.8509, 0.9254, 0.9984, 0.9736, 0.9727, 0.9894, 0.9872,\n",
       "        0.9812], device='cuda:1')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type_counts = adata.obs.groupby(['Cluster'])['Cluster'].count()\n",
    "cell_type_counts\n",
    "class_weights = [(1-(count/sum(cell_type_counts))) for count in cell_type_counts]\n",
    "class_weights = torch.FloatTensor(class_weights)\n",
    "class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "f1_scores = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "precision = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    \n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    _, actual = torch.max(y_test.data, 1)\n",
    "\n",
    "    correct_pred = (y_pred_tags == actual).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def multi_f1(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    _, actual = torch.max(y_test.data, 1)\n",
    "    return f1_score(actual.cpu(), y_pred_tags.cpu(), average='weighted', labels=np.unique(y_pred_tags.cpu()))\n",
    "\n",
    "\n",
    "def multi_precision(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    _, actual = torch.max(y_test.data, 1)\n",
    "    return precision_score(actual.cpu(), y_pred_tags.cpu(), average='weighted', labels=np.unique(y_pred_tags.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNAseqClassifier(\n",
       "  (linear1): Linear(in_features=10, out_features=64, bias=True)\n",
       "  (act_fn): ReLU()\n",
       "  (linear2): Linear(in_features=64, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEARNING_RATE = 0.007\n",
    "\n",
    "loss_module = nn.CrossEntropyLoss(weight = class_weights)\n",
    "\n",
    "loss_module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_cell_type(index):\n",
    "    arr = np.zeros(number_of_cell_types)\n",
    "    arr[index] = 1.0\n",
    "    arr = arr.reshape(1,-1)\n",
    "    return ohe.inverse_transform(arr)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training split 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414b4d6b365a40e4aa766bc724016bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) Epoch 001: | Train Loss: 0.91093 | Train Acc: 76.572\n",
      "(0) Epoch 002: | Train Loss: 0.39983 | Train Acc: 88.894\n",
      "(0) Epoch 003: | Train Loss: 0.32675 | Train Acc: 90.473\n",
      "(0) Epoch 004: | Train Loss: 0.29465 | Train Acc: 91.149\n",
      "(0) Epoch 005: | Train Loss: 0.27671 | Train Acc: 91.535\n",
      "(0) Epoch 006: | Train Loss: 0.26502 | Train Acc: 91.806\n",
      "(0) Epoch 007: | Train Loss: 0.25643 | Train Acc: 91.981\n",
      "(0) Epoch 008: | Train Loss: 0.24954 | Train Acc: 92.209\n",
      "(0) Epoch 009: | Train Loss: 0.24439 | Train Acc: 92.319\n",
      "(0) Epoch 010: | Train Loss: 0.23955 | Train Acc: 92.442\n",
      "(0) Epoch 011: | Train Loss: 0.23565 | Train Acc: 92.542\n",
      "(0) Epoch 012: | Train Loss: 0.23187 | Train Acc: 92.631\n",
      "(0) Epoch 013: | Train Loss: 0.22880 | Train Acc: 92.692\n",
      "(0) Epoch 014: | Train Loss: 0.22594 | Train Acc: 92.770\n",
      "(0) Epoch 015: | Train Loss: 0.22337 | Train Acc: 92.844\n",
      "(0) Epoch 016: | Train Loss: 0.22074 | Train Acc: 92.963\n",
      "(0) Epoch 017: | Train Loss: 0.21853 | Train Acc: 93.049\n",
      "(0) Epoch 018: | Train Loss: 0.21655 | Train Acc: 93.123\n",
      "(0) Epoch 019: | Train Loss: 0.21492 | Train Acc: 93.118\n",
      "(0) Epoch 020: | Train Loss: 0.21292 | Train Acc: 93.215\n",
      "(0) Epoch 021: | Train Loss: 0.21141 | Train Acc: 93.233\n",
      "(0) Epoch 022: | Train Loss: 0.20984 | Train Acc: 93.299\n",
      "(0) Epoch 023: | Train Loss: 0.20855 | Train Acc: 93.295\n",
      "(0) Epoch 024: | Train Loss: 0.20716 | Train Acc: 93.360\n",
      "(0) Epoch 025: | Train Loss: 0.20592 | Train Acc: 93.411\n",
      "(0) Epoch 026: | Train Loss: 0.20477 | Train Acc: 93.419\n",
      "(0) Epoch 027: | Train Loss: 0.20372 | Train Acc: 93.490\n",
      "(0) Epoch 028: | Train Loss: 0.20278 | Train Acc: 93.515\n",
      "(0) Epoch 029: | Train Loss: 0.20184 | Train Acc: 93.547\n",
      "(0) Epoch 030: | Train Loss: 0.20077 | Train Acc: 93.519\n",
      "(0) Epoch 031: | Train Loss: 0.20012 | Train Acc: 93.548\n",
      "(0) Epoch 032: | Train Loss: 0.19905 | Train Acc: 93.633\n",
      "(0) Epoch 033: | Train Loss: 0.19811 | Train Acc: 93.619\n",
      "(0) Epoch 034: | Train Loss: 0.19757 | Train Acc: 93.683\n",
      "(0) Epoch 035: | Train Loss: 0.19697 | Train Acc: 93.679\n",
      "(0) Epoch 036: | Train Loss: 0.19618 | Train Acc: 93.704\n",
      "(0) Epoch 037: | Train Loss: 0.19566 | Train Acc: 93.738\n",
      "(0) Epoch 038: | Train Loss: 0.19508 | Train Acc: 93.750\n",
      "(0) Epoch 039: | Train Loss: 0.19457 | Train Acc: 93.693\n",
      "(0) Epoch 040: | Train Loss: 0.19405 | Train Acc: 93.753\n",
      "(0) Epoch 041: | Train Loss: 0.19347 | Train Acc: 93.794\n",
      "(0) Epoch 042: | Train Loss: 0.19294 | Train Acc: 93.790\n",
      "(0) Epoch 043: | Train Loss: 0.19262 | Train Acc: 93.816\n",
      "(0) Epoch 044: | Train Loss: 0.19202 | Train Acc: 93.798\n",
      "(0) Epoch 045: | Train Loss: 0.19158 | Train Acc: 93.844\n",
      "(0) Epoch 046: | Train Loss: 0.19115 | Train Acc: 93.869\n",
      "(0) Epoch 047: | Train Loss: 0.19060 | Train Acc: 93.885\n",
      "(0) Epoch 048: | Train Loss: 0.19042 | Train Acc: 93.873\n",
      "(0) Epoch 049: | Train Loss: 0.18988 | Train Acc: 93.835\n",
      "(0) Epoch 050: | Train Loss: 0.18957 | Train Acc: 93.889\n",
      "(0 test accuracy = 0.9353208036292936)\n",
      "Begin training split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567ce0699cb442729675ec6c3281f0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Epoch 001: | Train Loss: 0.90872 | Train Acc: 76.583\n",
      "(1) Epoch 002: | Train Loss: 0.39242 | Train Acc: 89.191\n",
      "(1) Epoch 003: | Train Loss: 0.32192 | Train Acc: 90.774\n",
      "(1) Epoch 004: | Train Loss: 0.29086 | Train Acc: 91.349\n",
      "(1) Epoch 005: | Train Loss: 0.27361 | Train Acc: 91.703\n",
      "(1) Epoch 006: | Train Loss: 0.26194 | Train Acc: 92.017\n",
      "(1) Epoch 007: | Train Loss: 0.25360 | Train Acc: 92.182\n",
      "(1) Epoch 008: | Train Loss: 0.24697 | Train Acc: 92.329\n",
      "(1) Epoch 009: | Train Loss: 0.24131 | Train Acc: 92.443\n",
      "(1) Epoch 010: | Train Loss: 0.23673 | Train Acc: 92.546\n",
      "(1) Epoch 011: | Train Loss: 0.23275 | Train Acc: 92.694\n",
      "(1) Epoch 012: | Train Loss: 0.22913 | Train Acc: 92.742\n",
      "(1) Epoch 013: | Train Loss: 0.22579 | Train Acc: 92.880\n",
      "(1) Epoch 014: | Train Loss: 0.22313 | Train Acc: 92.907\n",
      "(1) Epoch 015: | Train Loss: 0.22040 | Train Acc: 92.985\n",
      "(1) Epoch 016: | Train Loss: 0.21819 | Train Acc: 93.076\n",
      "(1) Epoch 017: | Train Loss: 0.21613 | Train Acc: 93.123\n",
      "(1) Epoch 018: | Train Loss: 0.21398 | Train Acc: 93.230\n",
      "(1) Epoch 019: | Train Loss: 0.21232 | Train Acc: 93.214\n",
      "(1) Epoch 020: | Train Loss: 0.21046 | Train Acc: 93.310\n",
      "(1) Epoch 021: | Train Loss: 0.20909 | Train Acc: 93.342\n",
      "(1) Epoch 022: | Train Loss: 0.20754 | Train Acc: 93.389\n",
      "(1) Epoch 023: | Train Loss: 0.20638 | Train Acc: 93.431\n",
      "(1) Epoch 024: | Train Loss: 0.20513 | Train Acc: 93.441\n",
      "(1) Epoch 025: | Train Loss: 0.20379 | Train Acc: 93.534\n",
      "(1) Epoch 026: | Train Loss: 0.20307 | Train Acc: 93.565\n",
      "(1) Epoch 027: | Train Loss: 0.20200 | Train Acc: 93.571\n",
      "(1) Epoch 028: | Train Loss: 0.20109 | Train Acc: 93.572\n",
      "(1) Epoch 029: | Train Loss: 0.20030 | Train Acc: 93.599\n",
      "(1) Epoch 030: | Train Loss: 0.19952 | Train Acc: 93.619\n",
      "(1) Epoch 031: | Train Loss: 0.19882 | Train Acc: 93.645\n",
      "(1) Epoch 032: | Train Loss: 0.19802 | Train Acc: 93.628\n",
      "(1) Epoch 033: | Train Loss: 0.19732 | Train Acc: 93.658\n",
      "(1) Epoch 034: | Train Loss: 0.19661 | Train Acc: 93.661\n",
      "(1) Epoch 035: | Train Loss: 0.19600 | Train Acc: 93.675\n",
      "(1) Epoch 036: | Train Loss: 0.19544 | Train Acc: 93.681\n",
      "(1) Epoch 037: | Train Loss: 0.19485 | Train Acc: 93.773\n",
      "(1) Epoch 038: | Train Loss: 0.19453 | Train Acc: 93.736\n",
      "(1) Epoch 039: | Train Loss: 0.19389 | Train Acc: 93.718\n",
      "(1) Epoch 040: | Train Loss: 0.19334 | Train Acc: 93.792\n",
      "(1) Epoch 041: | Train Loss: 0.19267 | Train Acc: 93.754\n",
      "(1) Epoch 042: | Train Loss: 0.19225 | Train Acc: 93.773\n",
      "(1) Epoch 043: | Train Loss: 0.19183 | Train Acc: 93.790\n",
      "(1) Epoch 044: | Train Loss: 0.19136 | Train Acc: 93.823\n",
      "(1) Epoch 045: | Train Loss: 0.19110 | Train Acc: 93.849\n",
      "(1) Epoch 046: | Train Loss: 0.19041 | Train Acc: 93.864\n",
      "(1) Epoch 047: | Train Loss: 0.19010 | Train Acc: 93.906\n",
      "(1) Epoch 048: | Train Loss: 0.18993 | Train Acc: 93.806\n",
      "(1) Epoch 049: | Train Loss: 0.18944 | Train Acc: 93.833\n",
      "(1) Epoch 050: | Train Loss: 0.18886 | Train Acc: 93.912\n",
      "(1 test accuracy = 0.9329876863253402)\n",
      "Begin training split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8372a2ce30f0451d8fc26cc5f1b35fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) Epoch 001: | Train Loss: 0.88063 | Train Acc: 76.800\n",
      "(2) Epoch 002: | Train Loss: 0.38731 | Train Acc: 89.405\n",
      "(2) Epoch 003: | Train Loss: 0.31777 | Train Acc: 90.716\n",
      "(2) Epoch 004: | Train Loss: 0.28694 | Train Acc: 91.370\n",
      "(2) Epoch 005: | Train Loss: 0.26990 | Train Acc: 91.767\n",
      "(2) Epoch 006: | Train Loss: 0.25844 | Train Acc: 92.030\n",
      "(2) Epoch 007: | Train Loss: 0.25038 | Train Acc: 92.253\n",
      "(2) Epoch 008: | Train Loss: 0.24401 | Train Acc: 92.339\n",
      "(2) Epoch 009: | Train Loss: 0.23878 | Train Acc: 92.453\n",
      "(2) Epoch 010: | Train Loss: 0.23425 | Train Acc: 92.607\n",
      "(2) Epoch 011: | Train Loss: 0.23083 | Train Acc: 92.687\n",
      "(2) Epoch 012: | Train Loss: 0.22736 | Train Acc: 92.852\n",
      "(2) Epoch 013: | Train Loss: 0.22448 | Train Acc: 92.906\n",
      "(2) Epoch 014: | Train Loss: 0.22187 | Train Acc: 92.968\n",
      "(2) Epoch 015: | Train Loss: 0.21940 | Train Acc: 93.054\n",
      "(2) Epoch 016: | Train Loss: 0.21716 | Train Acc: 93.102\n",
      "(2) Epoch 017: | Train Loss: 0.21514 | Train Acc: 93.198\n",
      "(2) Epoch 018: | Train Loss: 0.21330 | Train Acc: 93.206\n",
      "(2) Epoch 019: | Train Loss: 0.21169 | Train Acc: 93.284\n",
      "(2) Epoch 020: | Train Loss: 0.21018 | Train Acc: 93.290\n",
      "(2) Epoch 021: | Train Loss: 0.20859 | Train Acc: 93.358\n",
      "(2) Epoch 022: | Train Loss: 0.20703 | Train Acc: 93.372\n",
      "(2) Epoch 023: | Train Loss: 0.20571 | Train Acc: 93.490\n",
      "(2) Epoch 024: | Train Loss: 0.20461 | Train Acc: 93.464\n",
      "(2) Epoch 025: | Train Loss: 0.20343 | Train Acc: 93.515\n",
      "(2) Epoch 026: | Train Loss: 0.20210 | Train Acc: 93.530\n",
      "(2) Epoch 027: | Train Loss: 0.20121 | Train Acc: 93.554\n",
      "(2) Epoch 028: | Train Loss: 0.20035 | Train Acc: 93.597\n",
      "(2) Epoch 029: | Train Loss: 0.19940 | Train Acc: 93.603\n",
      "(2) Epoch 030: | Train Loss: 0.19836 | Train Acc: 93.648\n",
      "(2) Epoch 031: | Train Loss: 0.19761 | Train Acc: 93.687\n",
      "(2) Epoch 032: | Train Loss: 0.19672 | Train Acc: 93.653\n",
      "(2) Epoch 033: | Train Loss: 0.19601 | Train Acc: 93.717\n",
      "(2) Epoch 034: | Train Loss: 0.19548 | Train Acc: 93.733\n",
      "(2) Epoch 035: | Train Loss: 0.19479 | Train Acc: 93.745\n",
      "(2) Epoch 036: | Train Loss: 0.19393 | Train Acc: 93.779\n",
      "(2) Epoch 037: | Train Loss: 0.19356 | Train Acc: 93.778\n",
      "(2) Epoch 038: | Train Loss: 0.19301 | Train Acc: 93.792\n",
      "(2) Epoch 039: | Train Loss: 0.19248 | Train Acc: 93.797\n",
      "(2) Epoch 040: | Train Loss: 0.19202 | Train Acc: 93.791\n",
      "(2) Epoch 041: | Train Loss: 0.19136 | Train Acc: 93.824\n",
      "(2) Epoch 042: | Train Loss: 0.19109 | Train Acc: 93.824\n",
      "(2) Epoch 043: | Train Loss: 0.19050 | Train Acc: 93.874\n",
      "(2) Epoch 044: | Train Loss: 0.19005 | Train Acc: 93.834\n",
      "(2) Epoch 045: | Train Loss: 0.18959 | Train Acc: 93.918\n",
      "(2) Epoch 046: | Train Loss: 0.18936 | Train Acc: 93.888\n",
      "(2) Epoch 047: | Train Loss: 0.18878 | Train Acc: 93.946\n",
      "(2) Epoch 048: | Train Loss: 0.18853 | Train Acc: 93.930\n",
      "(2) Epoch 049: | Train Loss: 0.18816 | Train Acc: 93.943\n",
      "(2) Epoch 050: | Train Loss: 0.18776 | Train Acc: 93.926\n",
      "(2 test accuracy = 0.9344134802333117)\n",
      "Begin training split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca74e23c26284f01859ad9e3172806f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3) Epoch 001: | Train Loss: 0.92410 | Train Acc: 75.416\n",
      "(3) Epoch 002: | Train Loss: 0.40680 | Train Acc: 88.659\n",
      "(3) Epoch 003: | Train Loss: 0.32997 | Train Acc: 90.378\n",
      "(3) Epoch 004: | Train Loss: 0.29618 | Train Acc: 91.017\n",
      "(3) Epoch 005: | Train Loss: 0.27730 | Train Acc: 91.467\n",
      "(3) Epoch 006: | Train Loss: 0.26473 | Train Acc: 91.740\n",
      "(3) Epoch 007: | Train Loss: 0.25575 | Train Acc: 92.013\n",
      "(3) Epoch 008: | Train Loss: 0.24884 | Train Acc: 92.219\n",
      "(3) Epoch 009: | Train Loss: 0.24312 | Train Acc: 92.434\n",
      "(3) Epoch 010: | Train Loss: 0.23847 | Train Acc: 92.453\n",
      "(3) Epoch 011: | Train Loss: 0.23440 | Train Acc: 92.576\n",
      "(3) Epoch 012: | Train Loss: 0.23094 | Train Acc: 92.645\n",
      "(3) Epoch 013: | Train Loss: 0.22787 | Train Acc: 92.772\n",
      "(3) Epoch 014: | Train Loss: 0.22513 | Train Acc: 92.766\n",
      "(3) Epoch 015: | Train Loss: 0.22274 | Train Acc: 92.899\n",
      "(3) Epoch 016: | Train Loss: 0.22024 | Train Acc: 92.963\n",
      "(3) Epoch 017: | Train Loss: 0.21840 | Train Acc: 93.021\n",
      "(3) Epoch 018: | Train Loss: 0.21630 | Train Acc: 93.128\n",
      "(3) Epoch 019: | Train Loss: 0.21477 | Train Acc: 93.140\n",
      "(3) Epoch 020: | Train Loss: 0.21315 | Train Acc: 93.223\n",
      "(3) Epoch 021: | Train Loss: 0.21137 | Train Acc: 93.274\n",
      "(3) Epoch 022: | Train Loss: 0.21001 | Train Acc: 93.376\n",
      "(3) Epoch 023: | Train Loss: 0.20886 | Train Acc: 93.367\n",
      "(3) Epoch 024: | Train Loss: 0.20742 | Train Acc: 93.380\n",
      "(3) Epoch 025: | Train Loss: 0.20640 | Train Acc: 93.360\n",
      "(3) Epoch 026: | Train Loss: 0.20499 | Train Acc: 93.467\n",
      "(3) Epoch 027: | Train Loss: 0.20408 | Train Acc: 93.470\n",
      "(3) Epoch 028: | Train Loss: 0.20300 | Train Acc: 93.505\n",
      "(3) Epoch 029: | Train Loss: 0.20216 | Train Acc: 93.536\n",
      "(3) Epoch 030: | Train Loss: 0.20113 | Train Acc: 93.609\n",
      "(3) Epoch 031: | Train Loss: 0.20021 | Train Acc: 93.631\n",
      "(3) Epoch 032: | Train Loss: 0.19935 | Train Acc: 93.596\n",
      "(3) Epoch 033: | Train Loss: 0.19860 | Train Acc: 93.625\n",
      "(3) Epoch 034: | Train Loss: 0.19791 | Train Acc: 93.659\n",
      "(3) Epoch 035: | Train Loss: 0.19724 | Train Acc: 93.694\n",
      "(3) Epoch 036: | Train Loss: 0.19642 | Train Acc: 93.736\n",
      "(3) Epoch 037: | Train Loss: 0.19578 | Train Acc: 93.713\n",
      "(3) Epoch 038: | Train Loss: 0.19493 | Train Acc: 93.711\n",
      "(3) Epoch 039: | Train Loss: 0.19445 | Train Acc: 93.774\n",
      "(3) Epoch 040: | Train Loss: 0.19378 | Train Acc: 93.762\n",
      "(3) Epoch 041: | Train Loss: 0.19319 | Train Acc: 93.794\n",
      "(3) Epoch 042: | Train Loss: 0.19268 | Train Acc: 93.829\n",
      "(3) Epoch 043: | Train Loss: 0.19214 | Train Acc: 93.847\n",
      "(3) Epoch 044: | Train Loss: 0.19167 | Train Acc: 93.862\n",
      "(3) Epoch 045: | Train Loss: 0.19096 | Train Acc: 93.846\n",
      "(3) Epoch 046: | Train Loss: 0.19022 | Train Acc: 93.896\n",
      "(3) Epoch 047: | Train Loss: 0.19025 | Train Acc: 93.892\n",
      "(3) Epoch 048: | Train Loss: 0.18945 | Train Acc: 93.923\n",
      "(3) Epoch 049: | Train Loss: 0.18907 | Train Acc: 93.879\n",
      "(3) Epoch 050: | Train Loss: 0.18863 | Train Acc: 93.926\n",
      "(3 test accuracy = 0.9336357744653273)\n",
      "Begin training split 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e962fa53659460d959a505c5062e6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) Epoch 001: | Train Loss: 0.92046 | Train Acc: 75.466\n",
      "(4) Epoch 002: | Train Loss: 0.40105 | Train Acc: 88.742\n",
      "(4) Epoch 003: | Train Loss: 0.32520 | Train Acc: 90.435\n",
      "(4) Epoch 004: | Train Loss: 0.29158 | Train Acc: 91.182\n",
      "(4) Epoch 005: | Train Loss: 0.27289 | Train Acc: 91.670\n",
      "(4) Epoch 006: | Train Loss: 0.26091 | Train Acc: 91.964\n",
      "(4) Epoch 007: | Train Loss: 0.25195 | Train Acc: 92.110\n",
      "(4) Epoch 008: | Train Loss: 0.24508 | Train Acc: 92.289\n",
      "(4) Epoch 009: | Train Loss: 0.23919 | Train Acc: 92.488\n",
      "(4) Epoch 010: | Train Loss: 0.23436 | Train Acc: 92.627\n",
      "(4) Epoch 011: | Train Loss: 0.23014 | Train Acc: 92.742\n",
      "(4) Epoch 012: | Train Loss: 0.22628 | Train Acc: 92.837\n",
      "(4) Epoch 013: | Train Loss: 0.22312 | Train Acc: 92.905\n",
      "(4) Epoch 014: | Train Loss: 0.22001 | Train Acc: 93.041\n",
      "(4) Epoch 015: | Train Loss: 0.21750 | Train Acc: 93.072\n",
      "(4) Epoch 016: | Train Loss: 0.21510 | Train Acc: 93.217\n",
      "(4) Epoch 017: | Train Loss: 0.21302 | Train Acc: 93.269\n",
      "(4) Epoch 018: | Train Loss: 0.21102 | Train Acc: 93.302\n",
      "(4) Epoch 019: | Train Loss: 0.20933 | Train Acc: 93.347\n",
      "(4) Epoch 020: | Train Loss: 0.20773 | Train Acc: 93.383\n",
      "(4) Epoch 021: | Train Loss: 0.20620 | Train Acc: 93.459\n",
      "(4) Epoch 022: | Train Loss: 0.20487 | Train Acc: 93.510\n",
      "(4) Epoch 023: | Train Loss: 0.20349 | Train Acc: 93.545\n",
      "(4) Epoch 024: | Train Loss: 0.20250 | Train Acc: 93.576\n",
      "(4) Epoch 025: | Train Loss: 0.20132 | Train Acc: 93.628\n",
      "(4) Epoch 026: | Train Loss: 0.20051 | Train Acc: 93.623\n",
      "(4) Epoch 027: | Train Loss: 0.19929 | Train Acc: 93.615\n",
      "(4) Epoch 028: | Train Loss: 0.19861 | Train Acc: 93.671\n",
      "(4) Epoch 029: | Train Loss: 0.19776 | Train Acc: 93.719\n",
      "(4) Epoch 030: | Train Loss: 0.19698 | Train Acc: 93.782\n",
      "(4) Epoch 031: | Train Loss: 0.19626 | Train Acc: 93.754\n",
      "(4) Epoch 032: | Train Loss: 0.19556 | Train Acc: 93.775\n",
      "(4) Epoch 033: | Train Loss: 0.19481 | Train Acc: 93.783\n",
      "(4) Epoch 034: | Train Loss: 0.19422 | Train Acc: 93.815\n",
      "(4) Epoch 035: | Train Loss: 0.19370 | Train Acc: 93.819\n",
      "(4) Epoch 036: | Train Loss: 0.19300 | Train Acc: 93.847\n",
      "(4) Epoch 037: | Train Loss: 0.19269 | Train Acc: 93.809\n",
      "(4) Epoch 038: | Train Loss: 0.19198 | Train Acc: 93.907\n",
      "(4) Epoch 039: | Train Loss: 0.19144 | Train Acc: 93.865\n",
      "(4) Epoch 040: | Train Loss: 0.19110 | Train Acc: 93.903\n",
      "(4) Epoch 041: | Train Loss: 0.19071 | Train Acc: 93.853\n",
      "(4) Epoch 042: | Train Loss: 0.19014 | Train Acc: 93.893\n",
      "(4) Epoch 043: | Train Loss: 0.18981 | Train Acc: 93.888\n",
      "(4) Epoch 044: | Train Loss: 0.18941 | Train Acc: 93.915\n",
      "(4) Epoch 045: | Train Loss: 0.18874 | Train Acc: 93.933\n",
      "(4) Epoch 046: | Train Loss: 0.18857 | Train Acc: 93.954\n",
      "(4) Epoch 047: | Train Loss: 0.18805 | Train Acc: 93.913\n",
      "(4) Epoch 048: | Train Loss: 0.18758 | Train Acc: 93.928\n",
      "(4) Epoch 049: | Train Loss: 0.18725 | Train Acc: 93.990\n",
      "(4) Epoch 050: | Train Loss: 0.18698 | Train Acc: 93.985\n",
      "(4 test accuracy = 0.9305249513933895)\n",
      "Begin training split 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e2cc469b5e461b80a6f3cb2daf48d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5) Epoch 001: | Train Loss: 0.90320 | Train Acc: 76.646\n",
      "(5) Epoch 002: | Train Loss: 0.39801 | Train Acc: 89.088\n",
      "(5) Epoch 003: | Train Loss: 0.32605 | Train Acc: 90.399\n",
      "(5) Epoch 004: | Train Loss: 0.29477 | Train Acc: 91.079\n",
      "(5) Epoch 005: | Train Loss: 0.27710 | Train Acc: 91.535\n",
      "(5) Epoch 006: | Train Loss: 0.26554 | Train Acc: 91.773\n",
      "(5) Epoch 007: | Train Loss: 0.25687 | Train Acc: 92.025\n",
      "(5) Epoch 008: | Train Loss: 0.25002 | Train Acc: 92.200\n",
      "(5) Epoch 009: | Train Loss: 0.24446 | Train Acc: 92.365\n",
      "(5) Epoch 010: | Train Loss: 0.23975 | Train Acc: 92.502\n",
      "(5) Epoch 011: | Train Loss: 0.23551 | Train Acc: 92.614\n",
      "(5) Epoch 012: | Train Loss: 0.23194 | Train Acc: 92.711\n",
      "(5) Epoch 013: | Train Loss: 0.22877 | Train Acc: 92.810\n",
      "(5) Epoch 014: | Train Loss: 0.22574 | Train Acc: 92.914\n",
      "(5) Epoch 015: | Train Loss: 0.22300 | Train Acc: 92.929\n",
      "(5) Epoch 016: | Train Loss: 0.22052 | Train Acc: 93.072\n",
      "(5) Epoch 017: | Train Loss: 0.21841 | Train Acc: 93.083\n",
      "(5) Epoch 018: | Train Loss: 0.21602 | Train Acc: 93.180\n",
      "(5) Epoch 019: | Train Loss: 0.21421 | Train Acc: 93.182\n",
      "(5) Epoch 020: | Train Loss: 0.21234 | Train Acc: 93.227\n",
      "(5) Epoch 021: | Train Loss: 0.21065 | Train Acc: 93.321\n",
      "(5) Epoch 022: | Train Loss: 0.20902 | Train Acc: 93.308\n",
      "(5) Epoch 023: | Train Loss: 0.20773 | Train Acc: 93.381\n",
      "(5) Epoch 024: | Train Loss: 0.20640 | Train Acc: 93.380\n",
      "(5) Epoch 025: | Train Loss: 0.20528 | Train Acc: 93.460\n",
      "(5) Epoch 026: | Train Loss: 0.20410 | Train Acc: 93.477\n",
      "(5) Epoch 027: | Train Loss: 0.20287 | Train Acc: 93.512\n",
      "(5) Epoch 028: | Train Loss: 0.20187 | Train Acc: 93.592\n",
      "(5) Epoch 029: | Train Loss: 0.20100 | Train Acc: 93.535\n",
      "(5) Epoch 030: | Train Loss: 0.20020 | Train Acc: 93.513\n",
      "(5) Epoch 031: | Train Loss: 0.19921 | Train Acc: 93.659\n",
      "(5) Epoch 032: | Train Loss: 0.19835 | Train Acc: 93.611\n",
      "(5) Epoch 033: | Train Loss: 0.19750 | Train Acc: 93.620\n",
      "(5) Epoch 034: | Train Loss: 0.19696 | Train Acc: 93.644\n",
      "(5) Epoch 035: | Train Loss: 0.19623 | Train Acc: 93.676\n",
      "(5) Epoch 036: | Train Loss: 0.19565 | Train Acc: 93.688\n",
      "(5) Epoch 037: | Train Loss: 0.19495 | Train Acc: 93.709\n",
      "(5) Epoch 038: | Train Loss: 0.19420 | Train Acc: 93.699\n",
      "(5) Epoch 039: | Train Loss: 0.19366 | Train Acc: 93.753\n",
      "(5) Epoch 040: | Train Loss: 0.19319 | Train Acc: 93.730\n",
      "(5) Epoch 041: | Train Loss: 0.19278 | Train Acc: 93.794\n",
      "(5) Epoch 042: | Train Loss: 0.19217 | Train Acc: 93.812\n",
      "(5) Epoch 043: | Train Loss: 0.19174 | Train Acc: 93.814\n",
      "(5) Epoch 044: | Train Loss: 0.19150 | Train Acc: 93.836\n",
      "(5) Epoch 045: | Train Loss: 0.19088 | Train Acc: 93.811\n",
      "(5) Epoch 046: | Train Loss: 0.19042 | Train Acc: 93.804\n",
      "(5) Epoch 047: | Train Loss: 0.19013 | Train Acc: 93.808\n",
      "(5) Epoch 048: | Train Loss: 0.18975 | Train Acc: 93.802\n",
      "(5) Epoch 049: | Train Loss: 0.18927 | Train Acc: 93.877\n",
      "(5) Epoch 050: | Train Loss: 0.18869 | Train Acc: 93.891\n",
      "(5 test accuracy = 0.9390797148412184)\n",
      "Begin training split 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4b5f80a49745289db26dd49461ebb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6) Epoch 001: | Train Loss: 0.90806 | Train Acc: 76.349\n",
      "(6) Epoch 002: | Train Loss: 0.40122 | Train Acc: 88.857\n",
      "(6) Epoch 003: | Train Loss: 0.32732 | Train Acc: 90.401\n",
      "(6) Epoch 004: | Train Loss: 0.29472 | Train Acc: 91.138\n",
      "(6) Epoch 005: | Train Loss: 0.27631 | Train Acc: 91.550\n",
      "(6) Epoch 006: | Train Loss: 0.26404 | Train Acc: 91.838\n",
      "(6) Epoch 007: | Train Loss: 0.25526 | Train Acc: 92.066\n",
      "(6) Epoch 008: | Train Loss: 0.24812 | Train Acc: 92.238\n",
      "(6) Epoch 009: | Train Loss: 0.24247 | Train Acc: 92.435\n",
      "(6) Epoch 010: | Train Loss: 0.23749 | Train Acc: 92.567\n",
      "(6) Epoch 011: | Train Loss: 0.23330 | Train Acc: 92.664\n",
      "(6) Epoch 012: | Train Loss: 0.22963 | Train Acc: 92.776\n",
      "(6) Epoch 013: | Train Loss: 0.22614 | Train Acc: 92.877\n",
      "(6) Epoch 014: | Train Loss: 0.22317 | Train Acc: 92.994\n",
      "(6) Epoch 015: | Train Loss: 0.22095 | Train Acc: 93.057\n",
      "(6) Epoch 016: | Train Loss: 0.21822 | Train Acc: 93.096\n",
      "(6) Epoch 017: | Train Loss: 0.21617 | Train Acc: 93.178\n",
      "(6) Epoch 018: | Train Loss: 0.21417 | Train Acc: 93.251\n",
      "(6) Epoch 019: | Train Loss: 0.21256 | Train Acc: 93.269\n",
      "(6) Epoch 020: | Train Loss: 0.21093 | Train Acc: 93.271\n",
      "(6) Epoch 021: | Train Loss: 0.20921 | Train Acc: 93.338\n",
      "(6) Epoch 022: | Train Loss: 0.20799 | Train Acc: 93.411\n",
      "(6) Epoch 023: | Train Loss: 0.20662 | Train Acc: 93.434\n",
      "(6) Epoch 024: | Train Loss: 0.20546 | Train Acc: 93.477\n",
      "(6) Epoch 025: | Train Loss: 0.20431 | Train Acc: 93.490\n",
      "(6) Epoch 026: | Train Loss: 0.20316 | Train Acc: 93.550\n",
      "(6) Epoch 027: | Train Loss: 0.20233 | Train Acc: 93.582\n",
      "(6) Epoch 028: | Train Loss: 0.20143 | Train Acc: 93.572\n",
      "(6) Epoch 029: | Train Loss: 0.20081 | Train Acc: 93.564\n",
      "(6) Epoch 030: | Train Loss: 0.19969 | Train Acc: 93.673\n",
      "(6) Epoch 031: | Train Loss: 0.19914 | Train Acc: 93.646\n",
      "(6) Epoch 032: | Train Loss: 0.19850 | Train Acc: 93.694\n",
      "(6) Epoch 033: | Train Loss: 0.19769 | Train Acc: 93.670\n",
      "(6) Epoch 034: | Train Loss: 0.19704 | Train Acc: 93.724\n",
      "(6) Epoch 035: | Train Loss: 0.19638 | Train Acc: 93.706\n",
      "(6) Epoch 036: | Train Loss: 0.19574 | Train Acc: 93.757\n",
      "(6) Epoch 037: | Train Loss: 0.19512 | Train Acc: 93.713\n",
      "(6) Epoch 038: | Train Loss: 0.19467 | Train Acc: 93.724\n",
      "(6) Epoch 039: | Train Loss: 0.19407 | Train Acc: 93.783\n",
      "(6) Epoch 040: | Train Loss: 0.19358 | Train Acc: 93.756\n",
      "(6) Epoch 041: | Train Loss: 0.19304 | Train Acc: 93.774\n",
      "(6) Epoch 042: | Train Loss: 0.19253 | Train Acc: 93.855\n",
      "(6) Epoch 043: | Train Loss: 0.19205 | Train Acc: 93.793\n",
      "(6) Epoch 044: | Train Loss: 0.19157 | Train Acc: 93.797\n",
      "(6) Epoch 045: | Train Loss: 0.19121 | Train Acc: 93.906\n",
      "(6) Epoch 046: | Train Loss: 0.19043 | Train Acc: 93.875\n",
      "(6) Epoch 047: | Train Loss: 0.19043 | Train Acc: 93.891\n",
      "(6) Epoch 048: | Train Loss: 0.18971 | Train Acc: 93.884\n",
      "(6) Epoch 049: | Train Loss: 0.18930 | Train Acc: 93.888\n",
      "(6) Epoch 050: | Train Loss: 0.18897 | Train Acc: 93.881\n",
      "(6 test accuracy = 0.9380347420274825)\n",
      "Begin training split 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdc237073f3473da0fe22ef0276f7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7) Epoch 001: | Train Loss: 0.92996 | Train Acc: 74.886\n",
      "(7) Epoch 002: | Train Loss: 0.40646 | Train Acc: 88.553\n",
      "(7) Epoch 003: | Train Loss: 0.32956 | Train Acc: 90.241\n",
      "(7) Epoch 004: | Train Loss: 0.29588 | Train Acc: 91.002\n",
      "(7) Epoch 005: | Train Loss: 0.27696 | Train Acc: 91.448\n",
      "(7) Epoch 006: | Train Loss: 0.26493 | Train Acc: 91.822\n",
      "(7) Epoch 007: | Train Loss: 0.25641 | Train Acc: 91.982\n",
      "(7) Epoch 008: | Train Loss: 0.24975 | Train Acc: 92.199\n",
      "(7) Epoch 009: | Train Loss: 0.24443 | Train Acc: 92.353\n",
      "(7) Epoch 010: | Train Loss: 0.24001 | Train Acc: 92.494\n",
      "(7) Epoch 011: | Train Loss: 0.23616 | Train Acc: 92.622\n",
      "(7) Epoch 012: | Train Loss: 0.23262 | Train Acc: 92.724\n",
      "(7) Epoch 013: | Train Loss: 0.22964 | Train Acc: 92.814\n",
      "(7) Epoch 014: | Train Loss: 0.22719 | Train Acc: 92.836\n",
      "(7) Epoch 015: | Train Loss: 0.22451 | Train Acc: 92.909\n",
      "(7) Epoch 016: | Train Loss: 0.22243 | Train Acc: 93.031\n",
      "(7) Epoch 017: | Train Loss: 0.22039 | Train Acc: 93.018\n",
      "(7) Epoch 018: | Train Loss: 0.21867 | Train Acc: 93.122\n",
      "(7) Epoch 019: | Train Loss: 0.21669 | Train Acc: 93.167\n",
      "(7) Epoch 020: | Train Loss: 0.21509 | Train Acc: 93.128\n",
      "(7) Epoch 021: | Train Loss: 0.21348 | Train Acc: 93.318\n",
      "(7) Epoch 022: | Train Loss: 0.21213 | Train Acc: 93.272\n",
      "(7) Epoch 023: | Train Loss: 0.21076 | Train Acc: 93.292\n",
      "(7) Epoch 024: | Train Loss: 0.20946 | Train Acc: 93.338\n",
      "(7) Epoch 025: | Train Loss: 0.20834 | Train Acc: 93.400\n",
      "(7) Epoch 026: | Train Loss: 0.20717 | Train Acc: 93.344\n",
      "(7) Epoch 027: | Train Loss: 0.20587 | Train Acc: 93.445\n",
      "(7) Epoch 028: | Train Loss: 0.20469 | Train Acc: 93.479\n",
      "(7) Epoch 029: | Train Loss: 0.20394 | Train Acc: 93.489\n",
      "(7) Epoch 030: | Train Loss: 0.20309 | Train Acc: 93.562\n",
      "(7) Epoch 031: | Train Loss: 0.20222 | Train Acc: 93.556\n",
      "(7) Epoch 032: | Train Loss: 0.20108 | Train Acc: 93.584\n",
      "(7) Epoch 033: | Train Loss: 0.20041 | Train Acc: 93.623\n",
      "(7) Epoch 034: | Train Loss: 0.19958 | Train Acc: 93.638\n",
      "(7) Epoch 035: | Train Loss: 0.19875 | Train Acc: 93.698\n",
      "(7) Epoch 036: | Train Loss: 0.19835 | Train Acc: 93.709\n",
      "(7) Epoch 037: | Train Loss: 0.19754 | Train Acc: 93.722\n",
      "(7) Epoch 038: | Train Loss: 0.19688 | Train Acc: 93.754\n",
      "(7) Epoch 039: | Train Loss: 0.19616 | Train Acc: 93.754\n",
      "(7) Epoch 040: | Train Loss: 0.19554 | Train Acc: 93.777\n",
      "(7) Epoch 041: | Train Loss: 0.19493 | Train Acc: 93.792\n",
      "(7) Epoch 042: | Train Loss: 0.19426 | Train Acc: 93.832\n",
      "(7) Epoch 043: | Train Loss: 0.19376 | Train Acc: 93.796\n",
      "(7) Epoch 044: | Train Loss: 0.19313 | Train Acc: 93.813\n",
      "(7) Epoch 045: | Train Loss: 0.19280 | Train Acc: 93.844\n",
      "(7) Epoch 046: | Train Loss: 0.19230 | Train Acc: 93.826\n",
      "(7) Epoch 047: | Train Loss: 0.19163 | Train Acc: 93.898\n",
      "(7) Epoch 048: | Train Loss: 0.19121 | Train Acc: 93.889\n",
      "(7) Epoch 049: | Train Loss: 0.19090 | Train Acc: 93.861\n",
      "(7) Epoch 050: | Train Loss: 0.19055 | Train Acc: 93.869\n",
      "(7 test accuracy = 0.9338864402385274)\n",
      "Begin training split 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdccf32e8e442ec836326bd1eee8045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) Epoch 001: | Train Loss: 0.93030 | Train Acc: 75.980\n",
      "(8) Epoch 002: | Train Loss: 0.39772 | Train Acc: 88.845\n",
      "(8) Epoch 003: | Train Loss: 0.32550 | Train Acc: 90.399\n",
      "(8) Epoch 004: | Train Loss: 0.29349 | Train Acc: 91.134\n",
      "(8) Epoch 005: | Train Loss: 0.27554 | Train Acc: 91.477\n",
      "(8) Epoch 006: | Train Loss: 0.26390 | Train Acc: 91.729\n",
      "(8) Epoch 007: | Train Loss: 0.25543 | Train Acc: 91.999\n",
      "(8) Epoch 008: | Train Loss: 0.24881 | Train Acc: 92.180\n",
      "(8) Epoch 009: | Train Loss: 0.24332 | Train Acc: 92.320\n",
      "(8) Epoch 010: | Train Loss: 0.23896 | Train Acc: 92.459\n",
      "(8) Epoch 011: | Train Loss: 0.23487 | Train Acc: 92.583\n",
      "(8) Epoch 012: | Train Loss: 0.23157 | Train Acc: 92.639\n",
      "(8) Epoch 013: | Train Loss: 0.22819 | Train Acc: 92.748\n",
      "(8) Epoch 014: | Train Loss: 0.22566 | Train Acc: 92.816\n",
      "(8) Epoch 015: | Train Loss: 0.22322 | Train Acc: 92.894\n",
      "(8) Epoch 016: | Train Loss: 0.22099 | Train Acc: 92.969\n",
      "(8) Epoch 017: | Train Loss: 0.21878 | Train Acc: 93.056\n",
      "(8) Epoch 018: | Train Loss: 0.21691 | Train Acc: 93.077\n",
      "(8) Epoch 019: | Train Loss: 0.21510 | Train Acc: 93.178\n",
      "(8) Epoch 020: | Train Loss: 0.21366 | Train Acc: 93.183\n",
      "(8) Epoch 021: | Train Loss: 0.21196 | Train Acc: 93.208\n",
      "(8) Epoch 022: | Train Loss: 0.21062 | Train Acc: 93.285\n",
      "(8) Epoch 023: | Train Loss: 0.20914 | Train Acc: 93.320\n",
      "(8) Epoch 024: | Train Loss: 0.20777 | Train Acc: 93.376\n",
      "(8) Epoch 025: | Train Loss: 0.20693 | Train Acc: 93.393\n",
      "(8) Epoch 026: | Train Loss: 0.20573 | Train Acc: 93.429\n",
      "(8) Epoch 027: | Train Loss: 0.20481 | Train Acc: 93.448\n",
      "(8) Epoch 028: | Train Loss: 0.20350 | Train Acc: 93.557\n",
      "(8) Epoch 029: | Train Loss: 0.20274 | Train Acc: 93.529\n",
      "(8) Epoch 030: | Train Loss: 0.20200 | Train Acc: 93.525\n",
      "(8) Epoch 031: | Train Loss: 0.20100 | Train Acc: 93.518\n",
      "(8) Epoch 032: | Train Loss: 0.19991 | Train Acc: 93.617\n",
      "(8) Epoch 033: | Train Loss: 0.19971 | Train Acc: 93.613\n",
      "(8) Epoch 034: | Train Loss: 0.19865 | Train Acc: 93.673\n",
      "(8) Epoch 035: | Train Loss: 0.19784 | Train Acc: 93.655\n",
      "(8) Epoch 036: | Train Loss: 0.19741 | Train Acc: 93.712\n",
      "(8) Epoch 037: | Train Loss: 0.19665 | Train Acc: 93.650\n",
      "(8) Epoch 038: | Train Loss: 0.19591 | Train Acc: 93.715\n",
      "(8) Epoch 039: | Train Loss: 0.19554 | Train Acc: 93.738\n",
      "(8) Epoch 040: | Train Loss: 0.19481 | Train Acc: 93.735\n",
      "(8) Epoch 041: | Train Loss: 0.19419 | Train Acc: 93.762\n",
      "(8) Epoch 042: | Train Loss: 0.19369 | Train Acc: 93.762\n",
      "(8) Epoch 043: | Train Loss: 0.19334 | Train Acc: 93.778\n",
      "(8) Epoch 044: | Train Loss: 0.19296 | Train Acc: 93.785\n",
      "(8) Epoch 045: | Train Loss: 0.19239 | Train Acc: 93.854\n",
      "(8) Epoch 046: | Train Loss: 0.19194 | Train Acc: 93.854\n",
      "(8) Epoch 047: | Train Loss: 0.19144 | Train Acc: 93.850\n",
      "(8) Epoch 048: | Train Loss: 0.19108 | Train Acc: 93.774\n",
      "(8) Epoch 049: | Train Loss: 0.19058 | Train Acc: 93.891\n",
      "(8) Epoch 050: | Train Loss: 0.19024 | Train Acc: 93.867\n",
      "(8 test accuracy = 0.9360902255639098)\n",
      "Begin training split 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47403d17b9c1458fa620ef2b57948cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9) Epoch 001: | Train Loss: 0.90151 | Train Acc: 75.837\n",
      "(9) Epoch 002: | Train Loss: 0.39469 | Train Acc: 89.067\n",
      "(9) Epoch 003: | Train Loss: 0.32431 | Train Acc: 90.499\n",
      "(9) Epoch 004: | Train Loss: 0.29351 | Train Acc: 91.149\n",
      "(9) Epoch 005: | Train Loss: 0.27627 | Train Acc: 91.593\n",
      "(9) Epoch 006: | Train Loss: 0.26506 | Train Acc: 91.810\n",
      "(9) Epoch 007: | Train Loss: 0.25694 | Train Acc: 92.024\n",
      "(9) Epoch 008: | Train Loss: 0.25049 | Train Acc: 92.243\n",
      "(9) Epoch 009: | Train Loss: 0.24500 | Train Acc: 92.386\n",
      "(9) Epoch 010: | Train Loss: 0.24045 | Train Acc: 92.487\n",
      "(9) Epoch 011: | Train Loss: 0.23655 | Train Acc: 92.590\n",
      "(9) Epoch 012: | Train Loss: 0.23286 | Train Acc: 92.695\n",
      "(9) Epoch 013: | Train Loss: 0.22980 | Train Acc: 92.763\n",
      "(9) Epoch 014: | Train Loss: 0.22696 | Train Acc: 92.903\n",
      "(9) Epoch 015: | Train Loss: 0.22443 | Train Acc: 92.889\n",
      "(9) Epoch 016: | Train Loss: 0.22223 | Train Acc: 92.991\n",
      "(9) Epoch 017: | Train Loss: 0.21997 | Train Acc: 93.082\n",
      "(9) Epoch 018: | Train Loss: 0.21784 | Train Acc: 93.115\n",
      "(9) Epoch 019: | Train Loss: 0.21596 | Train Acc: 93.171\n",
      "(9) Epoch 020: | Train Loss: 0.21437 | Train Acc: 93.244\n",
      "(9) Epoch 021: | Train Loss: 0.21260 | Train Acc: 93.280\n",
      "(9) Epoch 022: | Train Loss: 0.21113 | Train Acc: 93.298\n",
      "(9) Epoch 023: | Train Loss: 0.20969 | Train Acc: 93.424\n",
      "(9) Epoch 024: | Train Loss: 0.20846 | Train Acc: 93.409\n",
      "(9) Epoch 025: | Train Loss: 0.20723 | Train Acc: 93.435\n",
      "(9) Epoch 026: | Train Loss: 0.20594 | Train Acc: 93.480\n",
      "(9) Epoch 027: | Train Loss: 0.20474 | Train Acc: 93.495\n",
      "(9) Epoch 028: | Train Loss: 0.20372 | Train Acc: 93.515\n",
      "(9) Epoch 029: | Train Loss: 0.20279 | Train Acc: 93.544\n",
      "(9) Epoch 030: | Train Loss: 0.20187 | Train Acc: 93.578\n",
      "(9) Epoch 031: | Train Loss: 0.20093 | Train Acc: 93.600\n",
      "(9) Epoch 032: | Train Loss: 0.20006 | Train Acc: 93.680\n",
      "(9) Epoch 033: | Train Loss: 0.19928 | Train Acc: 93.720\n",
      "(9) Epoch 034: | Train Loss: 0.19856 | Train Acc: 93.649\n",
      "(9) Epoch 035: | Train Loss: 0.19775 | Train Acc: 93.712\n",
      "(9) Epoch 036: | Train Loss: 0.19711 | Train Acc: 93.717\n",
      "(9) Epoch 037: | Train Loss: 0.19648 | Train Acc: 93.747\n",
      "(9) Epoch 038: | Train Loss: 0.19559 | Train Acc: 93.749\n",
      "(9) Epoch 039: | Train Loss: 0.19510 | Train Acc: 93.752\n",
      "(9) Epoch 040: | Train Loss: 0.19440 | Train Acc: 93.796\n",
      "(9) Epoch 041: | Train Loss: 0.19385 | Train Acc: 93.774\n",
      "(9) Epoch 042: | Train Loss: 0.19321 | Train Acc: 93.837\n",
      "(9) Epoch 043: | Train Loss: 0.19278 | Train Acc: 93.811\n",
      "(9) Epoch 044: | Train Loss: 0.19225 | Train Acc: 93.810\n",
      "(9) Epoch 045: | Train Loss: 0.19167 | Train Acc: 93.851\n",
      "(9) Epoch 046: | Train Loss: 0.19127 | Train Acc: 93.901\n",
      "(9) Epoch 047: | Train Loss: 0.19079 | Train Acc: 93.932\n",
      "(9) Epoch 048: | Train Loss: 0.19009 | Train Acc: 93.901\n",
      "(9) Epoch 049: | Train Loss: 0.18986 | Train Acc: 93.871\n",
      "(9) Epoch 050: | Train Loss: 0.18933 | Train Acc: 93.919\n",
      "(9 test accuracy = 0.9358309567021)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=77)\n",
    "cv_test_accuracy = []\n",
    "cv_test_precision = []\n",
    "cv_test_f1 = []\n",
    "cv_test_roc = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(adata.X, adata.obs['Cluster'])):\n",
    "    \n",
    "    model = SimpleRNAseqClassifier(num_inputs=adata.obsm[\"X_scANVI\"].shape[1], num_hidden=64, num_outputs=number_of_cell_types)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f'Begin training split {i}')\n",
    "    train_dataset = scRNAseqDataset(torch.from_numpy(adata.obsm[\"X_scANVI\"][train_index]), torch.from_numpy(labels_one_hot[train_index]))\n",
    "    test_dataset = scRNAseqDataset(torch.from_numpy(adata.obsm[\"X_scANVI\"][test_index]), torch.from_numpy(labels_one_hot[test_index]))\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS+1)):\n",
    "\n",
    "        #Create writer to use TensorBoard\n",
    "        writer = SummaryWriter('runs/model_CV_on_scanvi_5000_10_model')\n",
    "        model_plotted = False\n",
    "        \n",
    "        # TRAINING\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_acc = 0\n",
    "        train_epoch_f1 = 0\n",
    "        train_epoch_precision = 0\n",
    "        model.train()\n",
    "\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            #Push data to GPU\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "\n",
    "            if not model_plotted:\n",
    "                writer.add_graph(model, X_train_batch)\n",
    "                model_plotted = True\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_train_pred = model(X_train_batch)\n",
    "\n",
    "            #Calculate train metrics\n",
    "            train_loss = loss_module(y_train_pred, y_train_batch)\n",
    "            train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "            train_f1 = multi_f1(y_train_pred, y_train_batch)\n",
    "            train_precision = multi_precision(y_train_pred, y_train_batch)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_epoch_loss += train_loss.item()\n",
    "            train_epoch_acc += train_acc.item()\n",
    "            train_epoch_f1 += train_f1.item()\n",
    "            train_epoch_precision += train_precision.item()\n",
    "            \n",
    "            \n",
    "        # VALIDATION    \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "            writer.add_scalar('Loss/training', train_epoch_loss/len(train_loader), global_step = epoch + 1)\n",
    "            \n",
    "            f1_scores['train'].append(train_epoch_f1/len(train_loader))\n",
    "            writer.add_scalar('F1_Score/training', train_epoch_f1/len(train_loader), global_step = epoch + 1)\n",
    "\n",
    "            precision['train'].append(train_epoch_precision/len(train_loader))\n",
    "            writer.add_scalar('Precision/training', train_epoch_precision/len(train_loader), global_step = epoch + 1)\n",
    "\n",
    "            accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "            writer.add_scalar('Accuracy/training', train_epoch_acc/len(train_loader), global_step = epoch + 1)\n",
    "            \n",
    "        print(f'({i}) Epoch {epoch+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}')\n",
    "    \n",
    "    y_true_cells = []\n",
    "    y_pred_cells = []\n",
    "\n",
    "    y_true_prob = []\n",
    "    y_pred_prob = []\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    for X_test_batch, y_test_batch in test_loader:\n",
    "        #Push data to GPU\n",
    "        X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\t\n",
    "\n",
    "        y_true_prob.extend(y_test_batch)\n",
    "\n",
    "        _, actual = torch.max(y_test_batch.data, 1)\t\n",
    "\n",
    "        actural_cell_types = [index_to_cell_type(index) for index in actual.cpu().numpy()]\n",
    "\n",
    "        y_true_cells.extend(actural_cell_types)\t\n",
    "\n",
    "        outputs = model(X_test_batch)\t\n",
    "\n",
    "        y_pred_prob.extend(softmax(outputs))\n",
    "\n",
    "        #Czy tu nie powininem użyć softmax przed max???\n",
    "        _, predicted = torch.max(softmax(outputs), 1)\t\n",
    "\n",
    "        predicted_cell_types = [index_to_cell_type(index) for index in predicted.cpu().numpy()]\n",
    "\n",
    "        y_pred_cells.extend(predicted_cell_types)\n",
    "\n",
    "        \n",
    "    y_true_prob = [y.cpu().numpy() for y in y_true_prob]\n",
    "    y_pred_prob = [y.cpu().detach().numpy() for y in y_pred_prob]\n",
    "    acc = accuracy_score(y_true_cells, y_pred_cells)\n",
    "\n",
    "    print(f\"({i} test accuracy = {acc})\")\n",
    "    cv_test_accuracy.append(acc)\n",
    "    cv_test_precision.append(precision_score(y_true_cells,y_pred_cells, average='weighted'))\n",
    "    cv_test_f1.append(f1_score(y_true_cells, y_pred_cells, average='weighted'))\n",
    "    cv_test_roc.append(roc_auc_score(y_true_prob, y_pred_prob, average=\"weighted\", multi_class=\"ovr\"))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = list(zip(cv_test_accuracy, cv_test_precision, cv_test_f1, cv_test_roc))\n",
    "\n",
    "csv_file_path = '/home/brunopsz/Data/acc_prec_f1_roc_10_cv_scanvi_final'\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['accuracy', 'precision', 'f1', 'roc_auc']) \n",
    "    writer.writerows(data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9207599746649469\n",
      "0.9205126109593935\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_true_cells,y_pred_cells, average='weighted'))\n",
    "print(f1_score(y_true_cells, y_pred_cells, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9209332469215813"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true_cells, y_pred_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
