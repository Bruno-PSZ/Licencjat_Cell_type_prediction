{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 77146 × 20692\n",
       "    obs: 'barcode_name', 'Sample', 'Cluster'\n",
       "    var: 'gene_name'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.set_device(2)\n",
    "print(\"Device\", device)\n",
    "adata = ad.read_h5ad('/home/brunopsz/Data/GSE155249_COUNTS_NOT_NORMALIZED.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapomniałeś tego zrobić...\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.scale(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(\n",
    "    adata,\n",
    "    flavor=\"seurat_v3\",\n",
    "    n_top_genes=5000,\n",
    "    layer=\"counts\",\n",
    "    batch_key=\"Sample\",\n",
    "    subset=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['AT2, AT1 cells', 'B cells', 'CD4 CM T cells',\n",
      "       'CD4 cytotoxic T cells', 'CD4 prolif. T cells',\n",
      "       'CD8 cytotoxic T cells', 'CD8 cytotoxic TRM T cells',\n",
      "       'CD8 prolif. T cells', 'Ciliated cells', 'Club, Basal cells',\n",
      "       'DC1', 'DC2', 'Infected AT2, AT1 cells', 'Ionocytes', 'Mast cells',\n",
      "       'Migratory DC', 'Mixed myeloid', 'MoAM1', 'MoAM2', 'MoAM3',\n",
      "       'MoAM4', 'Plasma cells', 'Prolif. AM', 'TRAM1', 'TRAM2', 'Treg',\n",
      "       'iNKT cells', 'pDC'], dtype=object)]\n",
      "\n",
      "\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False).fit(adata.obs['Cluster'].to_numpy().reshape(-1,1))\n",
    "print(ohe.categories_)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "labels_one_hot = ohe.transform(adata.obs['Cluster'].to_numpy().reshape(-1,1))\n",
    "print(labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNAseqClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        # Initialize the modules we need to build the network\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class scRNAseqDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.label[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cell types = 28\n"
     ]
    }
   ],
   "source": [
    "cell_types = adata.obs['Cluster'].unique()\n",
    "number_of_cell_types = len(cell_types)\n",
    "print(\"Number of cell types = \" + str(number_of_cell_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    \n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    _, actual = torch.max(y_test.data, 1)\n",
    "\n",
    "    correct_pred = (y_pred_tags == actual).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def multi_f1(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    _, actual = torch.max(y_test.data, 1)\n",
    "    return f1_score(actual.cpu(), y_pred_tags.cpu(), average='weighted', labels=np.unique(y_pred_tags.cpu()))\n",
    "\n",
    "\n",
    "def multi_precision(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    _, actual = torch.max(y_test.data, 1)\n",
    "    return precision_score(actual.cpu(), y_pred_tags.cpu(), average='weighted', labels=np.unique(y_pred_tags.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "f1_scores = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "precision = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0007\n",
    "\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_cell_type(index):\n",
    "    arr = np.zeros(number_of_cell_types)\n",
    "    arr[index] = 1.0\n",
    "    arr = arr.reshape(1,-1)\n",
    "    return ohe.inverse_transform(arr)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training split 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d51dd0ef9942feab4eb9f56532d809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) Epoch 001: | Train Loss: 1.62181 | Train Acc: 57.725\n",
      "(0) Epoch 002: | Train Loss: 0.71866 | Train Acc: 80.159\n",
      "(0) Epoch 003: | Train Loss: 0.48861 | Train Acc: 86.442\n",
      "(0) Epoch 004: | Train Loss: 0.37577 | Train Acc: 89.935\n",
      "(0) Epoch 005: | Train Loss: 0.30633 | Train Acc: 92.065\n",
      "(0) Epoch 006: | Train Loss: 0.25806 | Train Acc: 93.514\n",
      "(0) Epoch 007: | Train Loss: 0.22174 | Train Acc: 94.676\n",
      "(0) Epoch 008: | Train Loss: 0.19303 | Train Acc: 95.598\n",
      "(0) Epoch 009: | Train Loss: 0.16963 | Train Acc: 96.324\n",
      "(0) Epoch 010: | Train Loss: 0.15008 | Train Acc: 96.971\n",
      "(0) Epoch 011: | Train Loss: 0.13350 | Train Acc: 97.505\n",
      "(0) Epoch 012: | Train Loss: 0.11937 | Train Acc: 97.965\n",
      "(0) Epoch 013: | Train Loss: 0.10721 | Train Acc: 98.284\n",
      "(0) Epoch 014: | Train Loss: 0.09672 | Train Acc: 98.587\n",
      "(0) Epoch 015: | Train Loss: 0.08765 | Train Acc: 98.844\n",
      "(0) Epoch 016: | Train Loss: 0.07974 | Train Acc: 99.034\n",
      "(0) Epoch 017: | Train Loss: 0.07281 | Train Acc: 99.206\n",
      "(0) Epoch 018: | Train Loss: 0.06679 | Train Acc: 99.332\n",
      "(0) Epoch 019: | Train Loss: 0.06141 | Train Acc: 99.447\n",
      "(0) Epoch 020: | Train Loss: 0.05670 | Train Acc: 99.521\n",
      "(0) Epoch 021: | Train Loss: 0.05250 | Train Acc: 99.582\n",
      "(0) Epoch 022: | Train Loss: 0.04875 | Train Acc: 99.650\n",
      "(0) Epoch 023: | Train Loss: 0.04541 | Train Acc: 99.696\n",
      "(0) Epoch 024: | Train Loss: 0.04242 | Train Acc: 99.735\n",
      "(0) Epoch 025: | Train Loss: 0.03969 | Train Acc: 99.777\n",
      "(0) Epoch 026: | Train Loss: 0.03725 | Train Acc: 99.804\n",
      "(0) Epoch 027: | Train Loss: 0.03503 | Train Acc: 99.826\n",
      "(0) Epoch 028: | Train Loss: 0.03301 | Train Acc: 99.855\n",
      "(0) Epoch 029: | Train Loss: 0.03118 | Train Acc: 99.867\n",
      "(0) Epoch 030: | Train Loss: 0.02950 | Train Acc: 99.887\n",
      "(0) Epoch 031: | Train Loss: 0.02796 | Train Acc: 99.900\n",
      "(0) Epoch 032: | Train Loss: 0.02654 | Train Acc: 99.918\n",
      "(0) Epoch 033: | Train Loss: 0.02524 | Train Acc: 99.925\n",
      "(0) Epoch 034: | Train Loss: 0.02404 | Train Acc: 99.932\n",
      "(0) Epoch 035: | Train Loss: 0.02293 | Train Acc: 99.946\n",
      "(0) Epoch 036: | Train Loss: 0.02190 | Train Acc: 99.954\n",
      "(0) Epoch 037: | Train Loss: 0.02095 | Train Acc: 99.959\n",
      "(0) Epoch 038: | Train Loss: 0.02006 | Train Acc: 99.963\n",
      "(0) Epoch 039: | Train Loss: 0.01923 | Train Acc: 99.968\n",
      "(0) Epoch 040: | Train Loss: 0.01845 | Train Acc: 99.975\n",
      "(0) Epoch 041: | Train Loss: 0.01774 | Train Acc: 99.974\n",
      "(0) Epoch 042: | Train Loss: 0.01706 | Train Acc: 99.976\n",
      "(0) Epoch 043: | Train Loss: 0.01642 | Train Acc: 99.979\n",
      "(0) Epoch 044: | Train Loss: 0.01583 | Train Acc: 99.979\n",
      "(0) Epoch 045: | Train Loss: 0.01526 | Train Acc: 99.982\n",
      "(0) Epoch 046: | Train Loss: 0.01474 | Train Acc: 99.985\n",
      "(0) Epoch 047: | Train Loss: 0.01424 | Train Acc: 99.986\n",
      "(0) Epoch 048: | Train Loss: 0.01377 | Train Acc: 99.986\n",
      "(0) Epoch 049: | Train Loss: 0.01333 | Train Acc: 99.986\n",
      "(0) Epoch 050: | Train Loss: 0.01291 | Train Acc: 99.989\n",
      "(0 test accuracy = 0.8836033700583279)\n",
      "Begin training split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ca0569fa7e4cb6bced37ce092ff49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Epoch 001: | Train Loss: 1.62282 | Train Acc: 60.571\n",
      "(1) Epoch 002: | Train Loss: 0.70221 | Train Acc: 79.847\n",
      "(1) Epoch 003: | Train Loss: 0.48417 | Train Acc: 86.513\n",
      "(1) Epoch 004: | Train Loss: 0.37430 | Train Acc: 90.014\n",
      "(1) Epoch 005: | Train Loss: 0.30505 | Train Acc: 92.063\n",
      "(1) Epoch 006: | Train Loss: 0.25648 | Train Acc: 93.581\n",
      "(1) Epoch 007: | Train Loss: 0.21995 | Train Acc: 94.725\n",
      "(1) Epoch 008: | Train Loss: 0.19098 | Train Acc: 95.652\n",
      "(1) Epoch 009: | Train Loss: 0.16741 | Train Acc: 96.443\n",
      "(1) Epoch 010: | Train Loss: 0.14779 | Train Acc: 97.100\n",
      "(1) Epoch 011: | Train Loss: 0.13131 | Train Acc: 97.594\n",
      "(1) Epoch 012: | Train Loss: 0.11725 | Train Acc: 98.018\n",
      "(1) Epoch 013: | Train Loss: 0.10521 | Train Acc: 98.384\n",
      "(1) Epoch 014: | Train Loss: 0.09483 | Train Acc: 98.685\n",
      "(1) Epoch 015: | Train Loss: 0.08592 | Train Acc: 98.895\n",
      "(1) Epoch 016: | Train Loss: 0.07811 | Train Acc: 99.091\n",
      "(1) Epoch 017: | Train Loss: 0.07131 | Train Acc: 99.241\n",
      "(1) Epoch 018: | Train Loss: 0.06537 | Train Acc: 99.367\n",
      "(1) Epoch 019: | Train Loss: 0.06012 | Train Acc: 99.473\n",
      "(1) Epoch 020: | Train Loss: 0.05549 | Train Acc: 99.567\n",
      "(1) Epoch 021: | Train Loss: 0.05137 | Train Acc: 99.639\n",
      "(1) Epoch 022: | Train Loss: 0.04772 | Train Acc: 99.690\n",
      "(1) Epoch 023: | Train Loss: 0.04443 | Train Acc: 99.733\n",
      "(1) Epoch 024: | Train Loss: 0.04148 | Train Acc: 99.766\n",
      "(1) Epoch 025: | Train Loss: 0.03882 | Train Acc: 99.801\n",
      "(1) Epoch 026: | Train Loss: 0.03643 | Train Acc: 99.829\n",
      "(1) Epoch 027: | Train Loss: 0.03427 | Train Acc: 99.847\n",
      "(1) Epoch 028: | Train Loss: 0.03229 | Train Acc: 99.866\n",
      "(1) Epoch 029: | Train Loss: 0.03049 | Train Acc: 99.876\n",
      "(1) Epoch 030: | Train Loss: 0.02885 | Train Acc: 99.898\n",
      "(1) Epoch 031: | Train Loss: 0.02734 | Train Acc: 99.909\n",
      "(1) Epoch 032: | Train Loss: 0.02596 | Train Acc: 99.918\n",
      "(1) Epoch 033: | Train Loss: 0.02469 | Train Acc: 99.935\n",
      "(1) Epoch 034: | Train Loss: 0.02351 | Train Acc: 99.953\n",
      "(1) Epoch 035: | Train Loss: 0.02243 | Train Acc: 99.952\n",
      "(1) Epoch 036: | Train Loss: 0.02143 | Train Acc: 99.956\n",
      "(1) Epoch 037: | Train Loss: 0.02049 | Train Acc: 99.964\n",
      "(1) Epoch 038: | Train Loss: 0.01962 | Train Acc: 99.970\n",
      "(1) Epoch 039: | Train Loss: 0.01882 | Train Acc: 99.974\n",
      "(1) Epoch 040: | Train Loss: 0.01806 | Train Acc: 99.976\n",
      "(1) Epoch 041: | Train Loss: 0.01735 | Train Acc: 99.981\n",
      "(1) Epoch 042: | Train Loss: 0.01669 | Train Acc: 99.985\n",
      "(1) Epoch 043: | Train Loss: 0.01608 | Train Acc: 99.986\n",
      "(1) Epoch 044: | Train Loss: 0.01549 | Train Acc: 99.986\n",
      "(1) Epoch 045: | Train Loss: 0.01495 | Train Acc: 99.988\n",
      "(1) Epoch 046: | Train Loss: 0.01443 | Train Acc: 99.990\n",
      "(1) Epoch 047: | Train Loss: 0.01394 | Train Acc: 99.990\n",
      "(1) Epoch 048: | Train Loss: 0.01349 | Train Acc: 99.990\n",
      "(1) Epoch 049: | Train Loss: 0.01305 | Train Acc: 99.990\n",
      "(1) Epoch 050: | Train Loss: 0.01265 | Train Acc: 99.990\n",
      "(1 test accuracy = 0.8864549578742709)\n",
      "Begin training split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db59253cc6d44d70a19ed8ade6ddf13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) Epoch 001: | Train Loss: 1.66664 | Train Acc: 61.141\n",
      "(2) Epoch 002: | Train Loss: 0.67644 | Train Acc: 80.994\n",
      "(2) Epoch 003: | Train Loss: 0.47277 | Train Acc: 86.759\n",
      "(2) Epoch 004: | Train Loss: 0.36840 | Train Acc: 90.012\n",
      "(2) Epoch 005: | Train Loss: 0.30189 | Train Acc: 92.153\n",
      "(2) Epoch 006: | Train Loss: 0.25480 | Train Acc: 93.629\n",
      "(2) Epoch 007: | Train Loss: 0.21914 | Train Acc: 94.739\n",
      "(2) Epoch 008: | Train Loss: 0.19088 | Train Acc: 95.669\n",
      "(2) Epoch 009: | Train Loss: 0.16764 | Train Acc: 96.439\n",
      "(2) Epoch 010: | Train Loss: 0.14830 | Train Acc: 97.094\n",
      "(2) Epoch 011: | Train Loss: 0.13187 | Train Acc: 97.625\n",
      "(2) Epoch 012: | Train Loss: 0.11782 | Train Acc: 98.052\n",
      "(2) Epoch 013: | Train Loss: 0.10572 | Train Acc: 98.390\n",
      "(2) Epoch 014: | Train Loss: 0.09527 | Train Acc: 98.702\n",
      "(2) Epoch 015: | Train Loss: 0.08623 | Train Acc: 98.927\n",
      "(2) Epoch 016: | Train Loss: 0.07837 | Train Acc: 99.096\n",
      "(2) Epoch 017: | Train Loss: 0.07149 | Train Acc: 99.255\n",
      "(2) Epoch 018: | Train Loss: 0.06545 | Train Acc: 99.374\n",
      "(2) Epoch 019: | Train Loss: 0.06016 | Train Acc: 99.479\n",
      "(2) Epoch 020: | Train Loss: 0.05546 | Train Acc: 99.544\n",
      "(2) Epoch 021: | Train Loss: 0.05132 | Train Acc: 99.605\n",
      "(2) Epoch 022: | Train Loss: 0.04763 | Train Acc: 99.675\n",
      "(2) Epoch 023: | Train Loss: 0.04433 | Train Acc: 99.712\n",
      "(2) Epoch 024: | Train Loss: 0.04138 | Train Acc: 99.765\n",
      "(2) Epoch 025: | Train Loss: 0.03871 | Train Acc: 99.801\n",
      "(2) Epoch 026: | Train Loss: 0.03632 | Train Acc: 99.831\n",
      "(2) Epoch 027: | Train Loss: 0.03414 | Train Acc: 99.852\n",
      "(2) Epoch 028: | Train Loss: 0.03217 | Train Acc: 99.877\n",
      "(2) Epoch 029: | Train Loss: 0.03038 | Train Acc: 99.885\n",
      "(2) Epoch 030: | Train Loss: 0.02873 | Train Acc: 99.903\n",
      "(2) Epoch 031: | Train Loss: 0.02723 | Train Acc: 99.913\n",
      "(2) Epoch 032: | Train Loss: 0.02585 | Train Acc: 99.923\n",
      "(2) Epoch 033: | Train Loss: 0.02458 | Train Acc: 99.936\n",
      "(2) Epoch 034: | Train Loss: 0.02341 | Train Acc: 99.943\n",
      "(2) Epoch 035: | Train Loss: 0.02233 | Train Acc: 99.952\n",
      "(2) Epoch 036: | Train Loss: 0.02132 | Train Acc: 99.959\n",
      "(2) Epoch 037: | Train Loss: 0.02039 | Train Acc: 99.970\n",
      "(2) Epoch 038: | Train Loss: 0.01953 | Train Acc: 99.974\n",
      "(2) Epoch 039: | Train Loss: 0.01872 | Train Acc: 99.975\n",
      "(2) Epoch 040: | Train Loss: 0.01797 | Train Acc: 99.979\n",
      "(2) Epoch 041: | Train Loss: 0.01727 | Train Acc: 99.982\n",
      "(2) Epoch 042: | Train Loss: 0.01661 | Train Acc: 99.982\n",
      "(2) Epoch 043: | Train Loss: 0.01600 | Train Acc: 99.985\n",
      "(2) Epoch 044: | Train Loss: 0.01541 | Train Acc: 99.985\n",
      "(2) Epoch 045: | Train Loss: 0.01487 | Train Acc: 99.988\n",
      "(2) Epoch 046: | Train Loss: 0.01436 | Train Acc: 99.988\n",
      "(2) Epoch 047: | Train Loss: 0.01388 | Train Acc: 99.989\n",
      "(2) Epoch 048: | Train Loss: 0.01342 | Train Acc: 99.989\n",
      "(2) Epoch 049: | Train Loss: 0.01299 | Train Acc: 99.989\n",
      "(2) Epoch 050: | Train Loss: 0.01258 | Train Acc: 99.989\n",
      "(2 test accuracy = 0.8859364873622813)\n",
      "Begin training split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413f34f8f9c14f9185e4cae04f83d3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3) Epoch 001: | Train Loss: 1.61131 | Train Acc: 60.212\n",
      "(3) Epoch 002: | Train Loss: 0.68407 | Train Acc: 81.022\n",
      "(3) Epoch 003: | Train Loss: 0.47410 | Train Acc: 86.812\n",
      "(3) Epoch 004: | Train Loss: 0.36526 | Train Acc: 90.124\n",
      "(3) Epoch 005: | Train Loss: 0.29795 | Train Acc: 92.253\n",
      "(3) Epoch 006: | Train Loss: 0.25101 | Train Acc: 93.759\n",
      "(3) Epoch 007: | Train Loss: 0.21573 | Train Acc: 94.859\n",
      "(3) Epoch 008: | Train Loss: 0.18772 | Train Acc: 95.781\n",
      "(3) Epoch 009: | Train Loss: 0.16490 | Train Acc: 96.498\n",
      "(3) Epoch 010: | Train Loss: 0.14577 | Train Acc: 97.068\n",
      "(3) Epoch 011: | Train Loss: 0.12967 | Train Acc: 97.637\n",
      "(3) Epoch 012: | Train Loss: 0.11586 | Train Acc: 98.016\n",
      "(3) Epoch 013: | Train Loss: 0.10408 | Train Acc: 98.356\n",
      "(3) Epoch 014: | Train Loss: 0.09389 | Train Acc: 98.664\n",
      "(3) Epoch 015: | Train Loss: 0.08505 | Train Acc: 98.854\n",
      "(3) Epoch 016: | Train Loss: 0.07736 | Train Acc: 99.028\n",
      "(3) Epoch 017: | Train Loss: 0.07063 | Train Acc: 99.209\n",
      "(3) Epoch 018: | Train Loss: 0.06475 | Train Acc: 99.362\n",
      "(3) Epoch 019: | Train Loss: 0.05954 | Train Acc: 99.484\n",
      "(3) Epoch 020: | Train Loss: 0.05494 | Train Acc: 99.585\n",
      "(3) Epoch 021: | Train Loss: 0.05087 | Train Acc: 99.649\n",
      "(3) Epoch 022: | Train Loss: 0.04724 | Train Acc: 99.701\n",
      "(3) Epoch 023: | Train Loss: 0.04401 | Train Acc: 99.748\n",
      "(3) Epoch 024: | Train Loss: 0.04108 | Train Acc: 99.783\n",
      "(3) Epoch 025: | Train Loss: 0.03846 | Train Acc: 99.812\n",
      "(3) Epoch 026: | Train Loss: 0.03609 | Train Acc: 99.827\n",
      "(3) Epoch 027: | Train Loss: 0.03393 | Train Acc: 99.848\n",
      "(3) Epoch 028: | Train Loss: 0.03198 | Train Acc: 99.863\n",
      "(3) Epoch 029: | Train Loss: 0.03020 | Train Acc: 99.878\n",
      "(3) Epoch 030: | Train Loss: 0.02857 | Train Acc: 99.896\n",
      "(3) Epoch 031: | Train Loss: 0.02708 | Train Acc: 99.912\n",
      "(3) Epoch 032: | Train Loss: 0.02570 | Train Acc: 99.923\n",
      "(3) Epoch 033: | Train Loss: 0.02444 | Train Acc: 99.928\n",
      "(3) Epoch 034: | Train Loss: 0.02327 | Train Acc: 99.932\n",
      "(3) Epoch 035: | Train Loss: 0.02220 | Train Acc: 99.943\n",
      "(3) Epoch 036: | Train Loss: 0.02120 | Train Acc: 99.950\n",
      "(3) Epoch 037: | Train Loss: 0.02028 | Train Acc: 99.960\n",
      "(3) Epoch 038: | Train Loss: 0.01941 | Train Acc: 99.963\n",
      "(3) Epoch 039: | Train Loss: 0.01861 | Train Acc: 99.970\n",
      "(3) Epoch 040: | Train Loss: 0.01786 | Train Acc: 99.979\n",
      "(3) Epoch 041: | Train Loss: 0.01716 | Train Acc: 99.983\n",
      "(3) Epoch 042: | Train Loss: 0.01651 | Train Acc: 99.985\n",
      "(3) Epoch 043: | Train Loss: 0.01589 | Train Acc: 99.988\n",
      "(3) Epoch 044: | Train Loss: 0.01532 | Train Acc: 99.989\n",
      "(3) Epoch 045: | Train Loss: 0.01478 | Train Acc: 99.989\n",
      "(3) Epoch 046: | Train Loss: 0.01427 | Train Acc: 99.989\n",
      "(3) Epoch 047: | Train Loss: 0.01378 | Train Acc: 99.989\n",
      "(3) Epoch 048: | Train Loss: 0.01333 | Train Acc: 99.990\n",
      "(3) Epoch 049: | Train Loss: 0.01290 | Train Acc: 99.992\n",
      "(3) Epoch 050: | Train Loss: 0.01250 | Train Acc: 99.992\n",
      "(3 test accuracy = 0.8841218405703176)\n",
      "Begin training split 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ccbc9a41ed4c389379950d8406d098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) Epoch 001: | Train Loss: 1.61725 | Train Acc: 59.700\n",
      "(4) Epoch 002: | Train Loss: 0.70548 | Train Acc: 79.426\n",
      "(4) Epoch 003: | Train Loss: 0.49001 | Train Acc: 86.126\n",
      "(4) Epoch 004: | Train Loss: 0.37850 | Train Acc: 89.853\n",
      "(4) Epoch 005: | Train Loss: 0.30846 | Train Acc: 92.087\n",
      "(4) Epoch 006: | Train Loss: 0.25962 | Train Acc: 93.553\n",
      "(4) Epoch 007: | Train Loss: 0.22289 | Train Acc: 94.747\n",
      "(4) Epoch 008: | Train Loss: 0.19387 | Train Acc: 95.653\n",
      "(4) Epoch 009: | Train Loss: 0.17010 | Train Acc: 96.418\n",
      "(4) Epoch 010: | Train Loss: 0.15038 | Train Acc: 97.070\n",
      "(4) Epoch 011: | Train Loss: 0.13358 | Train Acc: 97.609\n",
      "(4) Epoch 012: | Train Loss: 0.11924 | Train Acc: 98.030\n",
      "(4) Epoch 013: | Train Loss: 0.10694 | Train Acc: 98.389\n",
      "(4) Epoch 014: | Train Loss: 0.09633 | Train Acc: 98.669\n",
      "(4) Epoch 015: | Train Loss: 0.08715 | Train Acc: 98.913\n",
      "(4) Epoch 016: | Train Loss: 0.07916 | Train Acc: 99.094\n",
      "(4) Epoch 017: | Train Loss: 0.07215 | Train Acc: 99.251\n",
      "(4) Epoch 018: | Train Loss: 0.06605 | Train Acc: 99.350\n",
      "(4) Epoch 019: | Train Loss: 0.06068 | Train Acc: 99.457\n",
      "(4) Epoch 020: | Train Loss: 0.05593 | Train Acc: 99.545\n",
      "(4) Epoch 021: | Train Loss: 0.05172 | Train Acc: 99.638\n",
      "(4) Epoch 022: | Train Loss: 0.04800 | Train Acc: 99.693\n",
      "(4) Epoch 023: | Train Loss: 0.04466 | Train Acc: 99.730\n",
      "(4) Epoch 024: | Train Loss: 0.04166 | Train Acc: 99.767\n",
      "(4) Epoch 025: | Train Loss: 0.03897 | Train Acc: 99.798\n",
      "(4) Epoch 026: | Train Loss: 0.03654 | Train Acc: 99.827\n",
      "(4) Epoch 027: | Train Loss: 0.03435 | Train Acc: 99.852\n",
      "(4) Epoch 028: | Train Loss: 0.03234 | Train Acc: 99.874\n",
      "(4) Epoch 029: | Train Loss: 0.03053 | Train Acc: 99.887\n",
      "(4) Epoch 030: | Train Loss: 0.02887 | Train Acc: 99.898\n",
      "(4) Epoch 031: | Train Loss: 0.02735 | Train Acc: 99.923\n",
      "(4) Epoch 032: | Train Loss: 0.02595 | Train Acc: 99.932\n",
      "(4) Epoch 033: | Train Loss: 0.02467 | Train Acc: 99.946\n",
      "(4) Epoch 034: | Train Loss: 0.02349 | Train Acc: 99.960\n",
      "(4) Epoch 035: | Train Loss: 0.02239 | Train Acc: 99.965\n",
      "(4) Epoch 036: | Train Loss: 0.02138 | Train Acc: 99.967\n",
      "(4) Epoch 037: | Train Loss: 0.02044 | Train Acc: 99.970\n",
      "(4) Epoch 038: | Train Loss: 0.01957 | Train Acc: 99.975\n",
      "(4) Epoch 039: | Train Loss: 0.01876 | Train Acc: 99.979\n",
      "(4) Epoch 040: | Train Loss: 0.01801 | Train Acc: 99.981\n",
      "(4) Epoch 041: | Train Loss: 0.01729 | Train Acc: 99.982\n",
      "(4) Epoch 042: | Train Loss: 0.01663 | Train Acc: 99.982\n",
      "(4) Epoch 043: | Train Loss: 0.01601 | Train Acc: 99.988\n",
      "(4) Epoch 044: | Train Loss: 0.01543 | Train Acc: 99.989\n",
      "(4) Epoch 045: | Train Loss: 0.01488 | Train Acc: 99.990\n",
      "(4) Epoch 046: | Train Loss: 0.01437 | Train Acc: 99.994\n",
      "(4) Epoch 047: | Train Loss: 0.01388 | Train Acc: 99.994\n",
      "(4) Epoch 048: | Train Loss: 0.01343 | Train Acc: 99.996\n",
      "(4) Epoch 049: | Train Loss: 0.01299 | Train Acc: 99.996\n",
      "(4) Epoch 050: | Train Loss: 0.01259 | Train Acc: 99.996\n",
      "(4 test accuracy = 0.8839922229423202)\n",
      "Begin training split 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011f1b6361b2452897ce4562d5cbb420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5) Epoch 001: | Train Loss: 1.62370 | Train Acc: 60.496\n",
      "(5) Epoch 002: | Train Loss: 0.68716 | Train Acc: 81.075\n",
      "(5) Epoch 003: | Train Loss: 0.48071 | Train Acc: 86.319\n",
      "(5) Epoch 004: | Train Loss: 0.37376 | Train Acc: 89.739\n",
      "(5) Epoch 005: | Train Loss: 0.30618 | Train Acc: 91.925\n",
      "(5) Epoch 006: | Train Loss: 0.25861 | Train Acc: 93.509\n",
      "(5) Epoch 007: | Train Loss: 0.22256 | Train Acc: 94.644\n",
      "(5) Epoch 008: | Train Loss: 0.19406 | Train Acc: 95.623\n",
      "(5) Epoch 009: | Train Loss: 0.17072 | Train Acc: 96.330\n",
      "(5) Epoch 010: | Train Loss: 0.15123 | Train Acc: 96.943\n",
      "(5) Epoch 011: | Train Loss: 0.13476 | Train Acc: 97.504\n",
      "(5) Epoch 012: | Train Loss: 0.12069 | Train Acc: 97.931\n",
      "(5) Epoch 013: | Train Loss: 0.10854 | Train Acc: 98.257\n",
      "(5) Epoch 014: | Train Loss: 0.09804 | Train Acc: 98.591\n",
      "(5) Epoch 015: | Train Loss: 0.08893 | Train Acc: 98.805\n",
      "(5) Epoch 016: | Train Loss: 0.08100 | Train Acc: 99.008\n",
      "(5) Epoch 017: | Train Loss: 0.07399 | Train Acc: 99.179\n",
      "(5) Epoch 018: | Train Loss: 0.06788 | Train Acc: 99.316\n",
      "(5) Epoch 019: | Train Loss: 0.06246 | Train Acc: 99.421\n",
      "(5) Epoch 020: | Train Loss: 0.05766 | Train Acc: 99.522\n",
      "(5) Epoch 021: | Train Loss: 0.05342 | Train Acc: 99.591\n",
      "(5) Epoch 022: | Train Loss: 0.04960 | Train Acc: 99.642\n",
      "(5) Epoch 023: | Train Loss: 0.04620 | Train Acc: 99.699\n",
      "(5) Epoch 024: | Train Loss: 0.04314 | Train Acc: 99.728\n",
      "(5) Epoch 025: | Train Loss: 0.04038 | Train Acc: 99.761\n",
      "(5) Epoch 026: | Train Loss: 0.03789 | Train Acc: 99.788\n",
      "(5) Epoch 027: | Train Loss: 0.03562 | Train Acc: 99.811\n",
      "(5) Epoch 028: | Train Loss: 0.03356 | Train Acc: 99.837\n",
      "(5) Epoch 029: | Train Loss: 0.03168 | Train Acc: 99.853\n",
      "(5) Epoch 030: | Train Loss: 0.02995 | Train Acc: 99.876\n",
      "(5) Epoch 031: | Train Loss: 0.02840 | Train Acc: 99.895\n",
      "(5) Epoch 032: | Train Loss: 0.02695 | Train Acc: 99.912\n",
      "(5) Epoch 033: | Train Loss: 0.02561 | Train Acc: 99.921\n",
      "(5) Epoch 034: | Train Loss: 0.02439 | Train Acc: 99.931\n",
      "(5) Epoch 035: | Train Loss: 0.02324 | Train Acc: 99.936\n",
      "(5) Epoch 036: | Train Loss: 0.02220 | Train Acc: 99.945\n",
      "(5) Epoch 037: | Train Loss: 0.02123 | Train Acc: 99.954\n",
      "(5) Epoch 038: | Train Loss: 0.02032 | Train Acc: 99.961\n",
      "(5) Epoch 039: | Train Loss: 0.01947 | Train Acc: 99.965\n",
      "(5) Epoch 040: | Train Loss: 0.01868 | Train Acc: 99.970\n",
      "(5) Epoch 041: | Train Loss: 0.01794 | Train Acc: 99.971\n",
      "(5) Epoch 042: | Train Loss: 0.01725 | Train Acc: 99.976\n",
      "(5) Epoch 043: | Train Loss: 0.01660 | Train Acc: 99.982\n",
      "(5) Epoch 044: | Train Loss: 0.01599 | Train Acc: 99.982\n",
      "(5) Epoch 045: | Train Loss: 0.01542 | Train Acc: 99.983\n",
      "(5) Epoch 046: | Train Loss: 0.01488 | Train Acc: 99.985\n",
      "(5) Epoch 047: | Train Loss: 0.01437 | Train Acc: 99.988\n",
      "(5) Epoch 048: | Train Loss: 0.01390 | Train Acc: 99.988\n",
      "(5) Epoch 049: | Train Loss: 0.01344 | Train Acc: 99.989\n",
      "(5) Epoch 050: | Train Loss: 0.01301 | Train Acc: 99.990\n",
      "(5 test accuracy = 0.8837329876863254)\n",
      "Begin training split 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff76fe2904b446d97c0f44e7baa2099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6) Epoch 001: | Train Loss: 1.60170 | Train Acc: 60.921\n",
      "(6) Epoch 002: | Train Loss: 0.67826 | Train Acc: 81.066\n",
      "(6) Epoch 003: | Train Loss: 0.47220 | Train Acc: 86.834\n",
      "(6) Epoch 004: | Train Loss: 0.36781 | Train Acc: 90.093\n",
      "(6) Epoch 005: | Train Loss: 0.30196 | Train Acc: 92.197\n",
      "(6) Epoch 006: | Train Loss: 0.25540 | Train Acc: 93.733\n",
      "(6) Epoch 007: | Train Loss: 0.21996 | Train Acc: 94.789\n",
      "(6) Epoch 008: | Train Loss: 0.19171 | Train Acc: 95.775\n",
      "(6) Epoch 009: | Train Loss: 0.16859 | Train Acc: 96.475\n",
      "(6) Epoch 010: | Train Loss: 0.14921 | Train Acc: 97.117\n",
      "(6) Epoch 011: | Train Loss: 0.13279 | Train Acc: 97.613\n",
      "(6) Epoch 012: | Train Loss: 0.11877 | Train Acc: 98.041\n",
      "(6) Epoch 013: | Train Loss: 0.10670 | Train Acc: 98.347\n",
      "(6) Epoch 014: | Train Loss: 0.09628 | Train Acc: 98.629\n",
      "(6) Epoch 015: | Train Loss: 0.08723 | Train Acc: 98.854\n",
      "(6) Epoch 016: | Train Loss: 0.07937 | Train Acc: 99.059\n",
      "(6) Epoch 017: | Train Loss: 0.07251 | Train Acc: 99.241\n",
      "(6) Epoch 018: | Train Loss: 0.06648 | Train Acc: 99.354\n",
      "(6) Epoch 019: | Train Loss: 0.06115 | Train Acc: 99.471\n",
      "(6) Epoch 020: | Train Loss: 0.05647 | Train Acc: 99.551\n",
      "(6) Epoch 021: | Train Loss: 0.05231 | Train Acc: 99.608\n",
      "(6) Epoch 022: | Train Loss: 0.04859 | Train Acc: 99.660\n",
      "(6) Epoch 023: | Train Loss: 0.04528 | Train Acc: 99.707\n",
      "(6) Epoch 024: | Train Loss: 0.04229 | Train Acc: 99.748\n",
      "(6) Epoch 025: | Train Loss: 0.03961 | Train Acc: 99.779\n",
      "(6) Epoch 026: | Train Loss: 0.03719 | Train Acc: 99.808\n",
      "(6) Epoch 027: | Train Loss: 0.03498 | Train Acc: 99.831\n",
      "(6) Epoch 028: | Train Loss: 0.03298 | Train Acc: 99.845\n",
      "(6) Epoch 029: | Train Loss: 0.03116 | Train Acc: 99.870\n",
      "(6) Epoch 030: | Train Loss: 0.02949 | Train Acc: 99.878\n",
      "(6) Epoch 031: | Train Loss: 0.02795 | Train Acc: 99.895\n",
      "(6) Epoch 032: | Train Loss: 0.02654 | Train Acc: 99.906\n",
      "(6) Epoch 033: | Train Loss: 0.02525 | Train Acc: 99.921\n",
      "(6) Epoch 034: | Train Loss: 0.02405 | Train Acc: 99.929\n",
      "(6) Epoch 035: | Train Loss: 0.02294 | Train Acc: 99.942\n",
      "(6) Epoch 036: | Train Loss: 0.02191 | Train Acc: 99.950\n",
      "(6) Epoch 037: | Train Loss: 0.02096 | Train Acc: 99.957\n",
      "(6) Epoch 038: | Train Loss: 0.02007 | Train Acc: 99.961\n",
      "(6) Epoch 039: | Train Loss: 0.01923 | Train Acc: 99.968\n",
      "(6) Epoch 040: | Train Loss: 0.01847 | Train Acc: 99.970\n",
      "(6) Epoch 041: | Train Loss: 0.01774 | Train Acc: 99.970\n",
      "(6) Epoch 042: | Train Loss: 0.01707 | Train Acc: 99.974\n",
      "(6) Epoch 043: | Train Loss: 0.01643 | Train Acc: 99.974\n",
      "(6) Epoch 044: | Train Loss: 0.01583 | Train Acc: 99.975\n",
      "(6) Epoch 045: | Train Loss: 0.01527 | Train Acc: 99.976\n",
      "(6) Epoch 046: | Train Loss: 0.01474 | Train Acc: 99.979\n",
      "(6) Epoch 047: | Train Loss: 0.01425 | Train Acc: 99.982\n",
      "(6) Epoch 048: | Train Loss: 0.01377 | Train Acc: 99.981\n",
      "(6) Epoch 049: | Train Loss: 0.01333 | Train Acc: 99.985\n",
      "(6) Epoch 050: | Train Loss: 0.01291 | Train Acc: 99.989\n",
      "(6 test accuracy = 0.886310604096448)\n",
      "Begin training split 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0858d96f0a40e8a4b0c34eee58dae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7) Epoch 001: | Train Loss: 1.64859 | Train Acc: 57.459\n",
      "(7) Epoch 002: | Train Loss: 0.71121 | Train Acc: 79.875\n",
      "(7) Epoch 003: | Train Loss: 0.49212 | Train Acc: 86.489\n",
      "(7) Epoch 004: | Train Loss: 0.37949 | Train Acc: 89.905\n",
      "(7) Epoch 005: | Train Loss: 0.30853 | Train Acc: 92.089\n",
      "(7) Epoch 006: | Train Loss: 0.25831 | Train Acc: 93.616\n",
      "(7) Epoch 007: | Train Loss: 0.22084 | Train Acc: 94.816\n",
      "(7) Epoch 008: | Train Loss: 0.19151 | Train Acc: 95.785\n",
      "(7) Epoch 009: | Train Loss: 0.16767 | Train Acc: 96.484\n",
      "(7) Epoch 010: | Train Loss: 0.14799 | Train Acc: 97.141\n",
      "(7) Epoch 011: | Train Loss: 0.13140 | Train Acc: 97.628\n",
      "(7) Epoch 012: | Train Loss: 0.11727 | Train Acc: 98.045\n",
      "(7) Epoch 013: | Train Loss: 0.10523 | Train Acc: 98.388\n",
      "(7) Epoch 014: | Train Loss: 0.09485 | Train Acc: 98.694\n",
      "(7) Epoch 015: | Train Loss: 0.08588 | Train Acc: 98.929\n",
      "(7) Epoch 016: | Train Loss: 0.07807 | Train Acc: 99.119\n",
      "(7) Epoch 017: | Train Loss: 0.07126 | Train Acc: 99.265\n",
      "(7) Epoch 018: | Train Loss: 0.06532 | Train Acc: 99.399\n",
      "(7) Epoch 019: | Train Loss: 0.06008 | Train Acc: 99.507\n",
      "(7) Epoch 020: | Train Loss: 0.05542 | Train Acc: 99.584\n",
      "(7) Epoch 021: | Train Loss: 0.05132 | Train Acc: 99.631\n",
      "(7) Epoch 022: | Train Loss: 0.04765 | Train Acc: 99.693\n",
      "(7) Epoch 023: | Train Loss: 0.04437 | Train Acc: 99.733\n",
      "(7) Epoch 024: | Train Loss: 0.04141 | Train Acc: 99.775\n",
      "(7) Epoch 025: | Train Loss: 0.03876 | Train Acc: 99.805\n",
      "(7) Epoch 026: | Train Loss: 0.03638 | Train Acc: 99.832\n",
      "(7) Epoch 027: | Train Loss: 0.03421 | Train Acc: 99.859\n",
      "(7) Epoch 028: | Train Loss: 0.03224 | Train Acc: 99.878\n",
      "(7) Epoch 029: | Train Loss: 0.03043 | Train Acc: 99.895\n",
      "(7) Epoch 030: | Train Loss: 0.02880 | Train Acc: 99.907\n",
      "(7) Epoch 031: | Train Loss: 0.02729 | Train Acc: 99.918\n",
      "(7) Epoch 032: | Train Loss: 0.02591 | Train Acc: 99.934\n",
      "(7) Epoch 033: | Train Loss: 0.02464 | Train Acc: 99.942\n",
      "(7) Epoch 034: | Train Loss: 0.02346 | Train Acc: 99.945\n",
      "(7) Epoch 035: | Train Loss: 0.02237 | Train Acc: 99.953\n",
      "(7) Epoch 036: | Train Loss: 0.02136 | Train Acc: 99.964\n",
      "(7) Epoch 037: | Train Loss: 0.02044 | Train Acc: 99.963\n",
      "(7) Epoch 038: | Train Loss: 0.01956 | Train Acc: 99.967\n",
      "(7) Epoch 039: | Train Loss: 0.01875 | Train Acc: 99.971\n",
      "(7) Epoch 040: | Train Loss: 0.01800 | Train Acc: 99.972\n",
      "(7) Epoch 041: | Train Loss: 0.01729 | Train Acc: 99.979\n",
      "(7) Epoch 042: | Train Loss: 0.01663 | Train Acc: 99.981\n",
      "(7) Epoch 043: | Train Loss: 0.01600 | Train Acc: 99.982\n",
      "(7) Epoch 044: | Train Loss: 0.01542 | Train Acc: 99.986\n",
      "(7) Epoch 045: | Train Loss: 0.01487 | Train Acc: 99.986\n",
      "(7) Epoch 046: | Train Loss: 0.01436 | Train Acc: 99.990\n",
      "(7) Epoch 047: | Train Loss: 0.01387 | Train Acc: 99.990\n",
      "(7) Epoch 048: | Train Loss: 0.01341 | Train Acc: 99.992\n",
      "(7) Epoch 049: | Train Loss: 0.01298 | Train Acc: 99.993\n",
      "(7) Epoch 050: | Train Loss: 0.01257 | Train Acc: 99.993\n",
      "(7 test accuracy = 0.8799585169821105)\n",
      "Begin training split 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73f05d7a35f4ba2818f1cc529fd7ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) Epoch 001: | Train Loss: 1.61838 | Train Acc: 58.876\n",
      "(8) Epoch 002: | Train Loss: 0.69385 | Train Acc: 80.758\n",
      "(8) Epoch 003: | Train Loss: 0.47784 | Train Acc: 86.666\n",
      "(8) Epoch 004: | Train Loss: 0.36888 | Train Acc: 89.968\n",
      "(8) Epoch 005: | Train Loss: 0.30164 | Train Acc: 92.115\n",
      "(8) Epoch 006: | Train Loss: 0.25457 | Train Acc: 93.577\n",
      "(8) Epoch 007: | Train Loss: 0.21894 | Train Acc: 94.688\n",
      "(8) Epoch 008: | Train Loss: 0.19067 | Train Acc: 95.667\n",
      "(8) Epoch 009: | Train Loss: 0.16753 | Train Acc: 96.382\n",
      "(8) Epoch 010: | Train Loss: 0.14816 | Train Acc: 97.063\n",
      "(8) Epoch 011: | Train Loss: 0.13184 | Train Acc: 97.608\n",
      "(8) Epoch 012: | Train Loss: 0.11790 | Train Acc: 98.009\n",
      "(8) Epoch 013: | Train Loss: 0.10595 | Train Acc: 98.356\n",
      "(8) Epoch 014: | Train Loss: 0.09560 | Train Acc: 98.631\n",
      "(8) Epoch 015: | Train Loss: 0.08669 | Train Acc: 98.887\n",
      "(8) Epoch 016: | Train Loss: 0.07891 | Train Acc: 99.085\n",
      "(8) Epoch 017: | Train Loss: 0.07210 | Train Acc: 99.209\n",
      "(8) Epoch 018: | Train Loss: 0.06615 | Train Acc: 99.351\n",
      "(8) Epoch 019: | Train Loss: 0.06087 | Train Acc: 99.444\n",
      "(8) Epoch 020: | Train Loss: 0.05618 | Train Acc: 99.545\n",
      "(8) Epoch 021: | Train Loss: 0.05205 | Train Acc: 99.602\n",
      "(8) Epoch 022: | Train Loss: 0.04833 | Train Acc: 99.663\n",
      "(8) Epoch 023: | Train Loss: 0.04504 | Train Acc: 99.699\n",
      "(8) Epoch 024: | Train Loss: 0.04205 | Train Acc: 99.746\n",
      "(8) Epoch 025: | Train Loss: 0.03936 | Train Acc: 99.772\n",
      "(8) Epoch 026: | Train Loss: 0.03694 | Train Acc: 99.806\n",
      "(8) Epoch 027: | Train Loss: 0.03475 | Train Acc: 99.836\n",
      "(8) Epoch 028: | Train Loss: 0.03274 | Train Acc: 99.853\n",
      "(8) Epoch 029: | Train Loss: 0.03092 | Train Acc: 99.876\n",
      "(8) Epoch 030: | Train Loss: 0.02925 | Train Acc: 99.891\n",
      "(8) Epoch 031: | Train Loss: 0.02772 | Train Acc: 99.907\n",
      "(8) Epoch 032: | Train Loss: 0.02632 | Train Acc: 99.916\n",
      "(8) Epoch 033: | Train Loss: 0.02502 | Train Acc: 99.928\n",
      "(8) Epoch 034: | Train Loss: 0.02383 | Train Acc: 99.939\n",
      "(8) Epoch 035: | Train Loss: 0.02273 | Train Acc: 99.957\n",
      "(8) Epoch 036: | Train Loss: 0.02171 | Train Acc: 99.965\n",
      "(8) Epoch 037: | Train Loss: 0.02076 | Train Acc: 99.970\n",
      "(8) Epoch 038: | Train Loss: 0.01988 | Train Acc: 99.974\n",
      "(8) Epoch 039: | Train Loss: 0.01906 | Train Acc: 99.978\n",
      "(8) Epoch 040: | Train Loss: 0.01829 | Train Acc: 99.983\n",
      "(8) Epoch 041: | Train Loss: 0.01757 | Train Acc: 99.985\n",
      "(8) Epoch 042: | Train Loss: 0.01690 | Train Acc: 99.988\n",
      "(8) Epoch 043: | Train Loss: 0.01627 | Train Acc: 99.990\n",
      "(8) Epoch 044: | Train Loss: 0.01568 | Train Acc: 99.990\n",
      "(8) Epoch 045: | Train Loss: 0.01513 | Train Acc: 99.992\n",
      "(8) Epoch 046: | Train Loss: 0.01461 | Train Acc: 99.993\n",
      "(8) Epoch 047: | Train Loss: 0.01411 | Train Acc: 99.994\n",
      "(8) Epoch 048: | Train Loss: 0.01365 | Train Acc: 99.994\n",
      "(8) Epoch 049: | Train Loss: 0.01321 | Train Acc: 99.994\n",
      "(8) Epoch 050: | Train Loss: 0.01279 | Train Acc: 99.994\n",
      "(8 test accuracy = 0.8861809696655432)\n",
      "Begin training split 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08286dd28bea4366ab0724def90b5756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9) Epoch 001: | Train Loss: 1.63113 | Train Acc: 59.811\n",
      "(9) Epoch 002: | Train Loss: 0.70406 | Train Acc: 80.969\n",
      "(9) Epoch 003: | Train Loss: 0.48579 | Train Acc: 86.459\n",
      "(9) Epoch 004: | Train Loss: 0.37368 | Train Acc: 89.870\n",
      "(9) Epoch 005: | Train Loss: 0.30411 | Train Acc: 91.941\n",
      "(9) Epoch 006: | Train Loss: 0.25549 | Train Acc: 93.578\n",
      "(9) Epoch 007: | Train Loss: 0.21913 | Train Acc: 94.774\n",
      "(9) Epoch 008: | Train Loss: 0.19059 | Train Acc: 95.709\n",
      "(9) Epoch 009: | Train Loss: 0.16730 | Train Acc: 96.439\n",
      "(9) Epoch 010: | Train Loss: 0.14796 | Train Acc: 97.107\n",
      "(9) Epoch 011: | Train Loss: 0.13159 | Train Acc: 97.628\n",
      "(9) Epoch 012: | Train Loss: 0.11770 | Train Acc: 98.003\n",
      "(9) Epoch 013: | Train Loss: 0.10575 | Train Acc: 98.341\n",
      "(9) Epoch 014: | Train Loss: 0.09545 | Train Acc: 98.630\n",
      "(9) Epoch 015: | Train Loss: 0.08651 | Train Acc: 98.851\n",
      "(9) Epoch 016: | Train Loss: 0.07870 | Train Acc: 99.069\n",
      "(9) Epoch 017: | Train Loss: 0.07189 | Train Acc: 99.229\n",
      "(9) Epoch 018: | Train Loss: 0.06589 | Train Acc: 99.354\n",
      "(9) Epoch 019: | Train Loss: 0.06060 | Train Acc: 99.444\n",
      "(9) Epoch 020: | Train Loss: 0.05593 | Train Acc: 99.524\n",
      "(9) Epoch 021: | Train Loss: 0.05179 | Train Acc: 99.600\n",
      "(9) Epoch 022: | Train Loss: 0.04808 | Train Acc: 99.660\n",
      "(9) Epoch 023: | Train Loss: 0.04476 | Train Acc: 99.712\n",
      "(9) Epoch 024: | Train Loss: 0.04179 | Train Acc: 99.748\n",
      "(9) Epoch 025: | Train Loss: 0.03911 | Train Acc: 99.783\n",
      "(9) Epoch 026: | Train Loss: 0.03669 | Train Acc: 99.801\n",
      "(9) Epoch 027: | Train Loss: 0.03449 | Train Acc: 99.830\n",
      "(9) Epoch 028: | Train Loss: 0.03248 | Train Acc: 99.856\n",
      "(9) Epoch 029: | Train Loss: 0.03066 | Train Acc: 99.881\n",
      "(9) Epoch 030: | Train Loss: 0.02901 | Train Acc: 99.896\n",
      "(9) Epoch 031: | Train Loss: 0.02748 | Train Acc: 99.916\n",
      "(9) Epoch 032: | Train Loss: 0.02608 | Train Acc: 99.918\n",
      "(9) Epoch 033: | Train Loss: 0.02480 | Train Acc: 99.936\n",
      "(9) Epoch 034: | Train Loss: 0.02360 | Train Acc: 99.949\n",
      "(9) Epoch 035: | Train Loss: 0.02251 | Train Acc: 99.954\n",
      "(9) Epoch 036: | Train Loss: 0.02150 | Train Acc: 99.961\n",
      "(9) Epoch 037: | Train Loss: 0.02055 | Train Acc: 99.965\n",
      "(9) Epoch 038: | Train Loss: 0.01968 | Train Acc: 99.970\n",
      "(9) Epoch 039: | Train Loss: 0.01886 | Train Acc: 99.972\n",
      "(9) Epoch 040: | Train Loss: 0.01810 | Train Acc: 99.972\n",
      "(9) Epoch 041: | Train Loss: 0.01739 | Train Acc: 99.979\n",
      "(9) Epoch 042: | Train Loss: 0.01672 | Train Acc: 99.979\n",
      "(9) Epoch 043: | Train Loss: 0.01609 | Train Acc: 99.981\n",
      "(9) Epoch 044: | Train Loss: 0.01551 | Train Acc: 99.983\n",
      "(9) Epoch 045: | Train Loss: 0.01496 | Train Acc: 99.983\n",
      "(9) Epoch 046: | Train Loss: 0.01444 | Train Acc: 99.988\n",
      "(9) Epoch 047: | Train Loss: 0.01395 | Train Acc: 99.988\n",
      "(9) Epoch 048: | Train Loss: 0.01349 | Train Acc: 99.989\n",
      "(9) Epoch 049: | Train Loss: 0.01305 | Train Acc: 99.989\n",
      "(9) Epoch 050: | Train Loss: 0.01264 | Train Acc: 99.989\n",
      "(9 test accuracy = 0.8865698729582577)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=77)\n",
    "cv_test_accuracy = []\n",
    "cv_test_precision = []\n",
    "cv_test_f1 = []\n",
    "cv_test_roc = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(adata.X, adata.obs['Cluster'])):\n",
    "    \n",
    "    model = SimpleRNAseqClassifier(num_inputs=adata.shape[1], num_hidden=64, num_outputs=number_of_cell_types)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f'Begin training split {i}')\n",
    "    train_dataset = scRNAseqDataset(torch.from_numpy(adata.X[train_index]), torch.from_numpy(labels_one_hot[train_index]))\n",
    "    test_dataset = scRNAseqDataset(torch.from_numpy(adata.X[test_index]), torch.from_numpy(labels_one_hot[test_index]))\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS+1)):\n",
    "\n",
    "        #Create writer to use TensorBoard\n",
    "        writer = SummaryWriter('runs/model_CV_on_base_model')\n",
    "        model_plotted = False\n",
    "        \n",
    "        # TRAINING\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_acc = 0\n",
    "        train_epoch_f1 = 0\n",
    "        train_epoch_precision = 0\n",
    "        model.train()\n",
    "\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            #Push data to GPU\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "\n",
    "            if not model_plotted:\n",
    "                writer.add_graph(model, X_train_batch)\n",
    "                model_plotted = True\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_train_pred = model(X_train_batch)\n",
    "\n",
    "            # #maybe?????\n",
    "            y_train_pred = torch.log_softmax(y_train_pred, dim = 1)\n",
    "\n",
    "            #Calculate train metrics\n",
    "            train_loss = loss_module(y_train_pred, y_train_batch)\n",
    "            train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "            train_f1 = multi_f1(y_train_pred, y_train_batch)\n",
    "            train_precision = multi_precision(y_train_pred, y_train_batch)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_epoch_loss += train_loss.item()\n",
    "            train_epoch_acc += train_acc.item()\n",
    "            train_epoch_f1 += train_f1.item()\n",
    "            train_epoch_precision += train_precision.item()\n",
    "            \n",
    "            \n",
    "        # VALIDATION    \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "            writer.add_scalar('Loss/training', train_epoch_loss/len(train_loader), global_step = epoch + 1)\n",
    "            \n",
    "            f1_scores['train'].append(train_epoch_f1/len(train_loader))\n",
    "            writer.add_scalar('F1_Score/training', train_epoch_f1/len(train_loader), global_step = epoch + 1)\n",
    "\n",
    "            precision['train'].append(train_epoch_precision/len(train_loader))\n",
    "            writer.add_scalar('Precision/training', train_epoch_precision/len(train_loader), global_step = epoch + 1)\n",
    "\n",
    "            accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "            writer.add_scalar('Accuracy/training', train_epoch_acc/len(train_loader), global_step = epoch + 1)\n",
    "            \n",
    "        print(f'({i}) Epoch {epoch+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}')\n",
    "    \n",
    "    y_true_cells = []\n",
    "    y_pred_cells = []\n",
    "\n",
    "    y_true_prob = []\n",
    "    y_pred_prob = []\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    for X_test_batch, y_test_batch in test_loader:\n",
    "        #Push data to GPU\n",
    "        X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\t\n",
    "\n",
    "        y_true_prob.extend(y_test_batch)\n",
    "\n",
    "        _, actual = torch.max(y_test_batch.data, 1)\t\n",
    "\n",
    "        actural_cell_types = [index_to_cell_type(index) for index in actual.cpu().numpy()]\n",
    "\n",
    "        y_true_cells.extend(actural_cell_types)\t\n",
    "\n",
    "        outputs = model(X_test_batch)\t\n",
    "\n",
    "        y_pred_prob.extend(softmax(outputs))\n",
    "\n",
    "        #Czy tu nie powininem użyć softmax przed max???\n",
    "        _, predicted = torch.max(softmax(outputs), 1)\t\n",
    "\n",
    "        predicted_cell_types = [index_to_cell_type(index) for index in predicted.cpu().numpy()]\n",
    "\n",
    "        y_pred_cells.extend(predicted_cell_types)\n",
    "\n",
    "        \n",
    "    y_true_prob = [y.cpu().numpy() for y in y_true_prob]\n",
    "    y_pred_prob = [y.cpu().detach().numpy() for y in y_pred_prob]\n",
    "    acc = accuracy_score(y_true_cells, y_pred_cells)\n",
    "\n",
    "    print(f\"({i} test accuracy = {acc})\")\n",
    "    cv_test_accuracy.append(acc)\n",
    "    cv_test_precision.append(precision_score(y_true_cells,y_pred_cells, average='weighted'))\n",
    "    cv_test_f1.append(f1_score(y_true_cells, y_pred_cells, average='weighted'))\n",
    "    cv_test_roc.append(roc_auc_score(y_true_prob, y_pred_prob, average=\"weighted\", multi_class=\"ovr\"))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9944082318810038,\n",
       " 0.9945067036905018,\n",
       " 0.9940895178350934,\n",
       " 0.9943080241115064,\n",
       " 0.993986281305609,\n",
       " 0.9939387059432859,\n",
       " 0.994417169876028,\n",
       " 0.9940096865711178,\n",
       " 0.9942670833312208,\n",
       " 0.9942455829537971]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_test_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8833304984852983,\n",
       " 0.8862925357536315,\n",
       " 0.8857507995152686,\n",
       " 0.8840204642926202,\n",
       " 0.8837404533736724,\n",
       " 0.8832387608811492,\n",
       " 0.8858592420851722,\n",
       " 0.8796493319598641,\n",
       " 0.8858261161543948,\n",
       " 0.8862438887676163]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_test_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
